{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2934477c7af628ff464554801f459dba",
     "grade": false,
     "grade_id": "cell-99f58b9017e8efe1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Image Processing Laboratory Notebooks</h2>\n",
    "<hr style=\"clear:both\">\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "This Juypter notebook is part of a series of computer laboratories which are designed\n",
    "to teach image-processing programming; they are running on the EPFL's Noto server. They are the practical complement of the theoretical lectures of the EPFL's Master course <b>Image Processing II</b> \n",
    "(<a href=\"https://moodle.epfl.ch/course/view.php?id=463\">MICRO-512</a>) taught by Dr. D. Sage, Dr. M. Liebling, Prof. M. Unser and Prof. D. Van de Ville.\n",
    "</p>\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "The project is funded by the Center for Digital Education and the School of Engineering. It is owned by the <a href=\"http://bigwww.epfl.ch/\">Biomedical Imaging Group</a>. \n",
    "The distribution or the reproduction of the notebook is strictly prohibited without the written consent of the authors.  &copy; EPFL 2021.\n",
    "</p>\n",
    "<p style=\"font-size:0.85em; margin:0px\"><b>Authors</b>: \n",
    "    <a href=\"mailto:pol.delaguilapla@epfl.ch\">Pol del Aguila Pla</a>, \n",
    "    <a href=\"mailto:kay.lachler@epfl.ch\">Kay Lächler</a>,\n",
    "    <a href=\"mailto:alejandro.nogueronaramburu@epfl.ch\">Alejandro Noguerón Arámburu</a>,\n",
    "    <a href=\"mailto:daniel.sage@epfl.ch\">Daniel Sage</a>,\n",
    "    <a href=\"mailto:jaejun.yoo@epfl.ch\">Jaejun Yoo</a>, and\n",
    "    <a href=\"mailto:kamil.seghrouchni@epfl.ch\">Kamil Seghrouchni</a>.\n",
    "     \n",
    "</p>\n",
    "<hr style=\"clear:both\">\n",
    "<h1>Lab 7.2: Neural Networks: Application</h1>\n",
    "<div style=\"background-color:#F0F0F0;padding:4px\">\n",
    "    <p style=\"margin:4px;\"><b>Released</b>: Thursday May 27, 2021</p>\n",
    "    <p style=\"margin:4px;\"><b>Submission</b>: <span style=\"color:red\">Friday June 11, 2021</span> (before 11:59PM) on <a href=\"https://moodle.epfl.ch/course/view.php?id=463\">Moodle</a></p>\n",
    "    <p style=\"margin:4px;\"><b>Grade weigth</b>: Lab 7 (28 points), 7.5 % of the overall grade</p>\n",
    "    <p style=\"margin:4px;\"><b>Remote help</b>: Thursday 3 June and Monday 7 June, 2021 on Zoom (see Moodle for link)</p>    \n",
    "    <p style=\"margin:4px;\"><b>Related lectures</b>: Chapter 11</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name: \n",
    "### SCIPER: \n",
    "\n",
    "Double-click on this cell and fill your name and SCIPER number. Then, run the cell below to verify your identity in Noto and set the seed for random results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e24ab6f2099ca85a90d1dda4e563a89",
     "grade": true,
     "grade_id": "cell-aa5326c6d1773ea8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIPER: 286557\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "# This line recovers your camipro number to mark the images with your ID\n",
    "uid = int(getpass.getuser().split('-')[2]) if len(getpass.getuser().split('-')) > 2 else ord(getpass.getuser()[0])\n",
    "print(f'SCIPER: {uid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b05edc355dd5540d26798f5f1c46b6fc",
     "grade": false,
     "grade_id": "cell-a6f05f88b2393ac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <a name=\"imports_\"></a> Imports\n",
    "In the next cell we import Python libraries we will use throughout the lab.\n",
    "<!-- , as well as the `IPLabViewer` class, created specifically for this course, which provides interactive image visualization based on the `ipywidgets` library: -->\n",
    "* [`matplotlib.pyplot`](https://matplotlib.org/3.2.2/api/_as_gen/matplotlib.pyplot.html), to display images,\n",
    "<!-- * [`ipywidgets`](https://ipywidgets.readthedocs.io/en/latest/), to make the image display interactive, -->\n",
    "* [`numpy`](https://numpy.org/doc/stable/reference/index.html), for mathematical operations on arrays,\n",
    "* [`torch`](https://pytorch.org/), for comparing the results of our manual implementation with pytorch autograd,\n",
    "* [`sklearn`](https://scikit-learn.org/stable/), for the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "<!-- We will then load the `IPLabViewer` class (see the documentation [here](https://github.com/Biomedical-Imaging-Group/IPLabImageViewer/wiki/Python-IPLabViewer()-Class) or run the Python command `help(viewer)` after loading the class). -->\n",
    "\n",
    "Finally, we load the images you will use in the exercise to test your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95ca54988850123014ff52e5e329fc69",
     "grade": false,
     "grade_id": "cell-e3e2552bf2b6a196",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cv2 as cv \n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import IPLabViewer() Class\n",
    "import sys\n",
    "sys.path.insert(0, 'lib')\n",
    "from iplabs import IPLabViewer as viewer\n",
    "\n",
    "%matplotlib widget\n",
    "# Fix random seeds for reproducible results\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Load the images\n",
    "input_img = cv.imread('images/input.png', cv.IMREAD_UNCHANGED)\n",
    "label_img = cv.imread('images/label.png', cv.IMREAD_UNCHANGED)\n",
    "input_img_test = cv.imread('images/test_input.tiff', cv.IMREAD_UNCHANGED)\n",
    "label_img_test = cv.imread('images/test_label.tiff', cv.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a01414a131de298b336acaf44f08c255",
     "grade": false,
     "grade_id": "cell-2e47d9f9ce3f6ea1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Neural Networks: Application (8 points)\n",
    "\n",
    "In this laboratory we propose to study some applications of neural networks, namely pixel classification. If you still have questions concerning either programming or the lab in general, don't hesitate to contact one of the TAs listed on [Moodle](https://moodle.epfl.ch/course/view.php?id=463).\n",
    "\n",
    "## Index    \n",
    "1. [Multiclass pixel classification](#1.-Multiclass-pixel-classification)\n",
    "    1. [Data preparation](#1.A.-Data-preparation)\n",
    "        1. [Creating the input and target dataset](#1.A.a.-Creating-the-input-and-target-dataset)\n",
    "        2. [Split the datasets into train and validation sets](#1.A.b.-Split-the-datasets-into-train-and-validation-sets)\n",
    "        3. [Creating the DataLoader](#1.A.c.-Creating-the-DataLoader)\n",
    "    2. [Softmax and cross-entropy loss for multiclass classification](#1.B.-Softmax-and-cross-entropy-loss-for-multiclass-classification)\n",
    "        1. [Softmax in NumPy](#1.B.a.-Softmax-in-NumPy-(1-point)) **(1 point)**\n",
    "        2. [Softmax in PyTorch](#1.B.b.-Softmax-in-PyTorch)\n",
    "        3. [Cross-entropy in NumPy](#1.B.c.-Cross-entropy-in-NumPy-(2-points)) **(2 points)**\n",
    "        4. [Cross-entropy in PyTorch](#1.B.d.-Cross-entropy-in-PyTorch)\n",
    "    3. [Build a model and train it with PyTorch](#1.C.-Build-a-model-and-train-it-with-PyTorch)\n",
    "        1. [Define the model](#1.C.a.-Define-the-model-(2-points)) **(2 points)**\n",
    "        2. [Complete the training pipeline](#1.C.b.-Complete-the-training-pipeline-(2-points)) **(2 points)**\n",
    "        3. [Test the model on a test image](#1.C.c-Test-the-model-on-a-test-image)\n",
    "        4. [Classwise accuracy](#1.C.d.-Classwise-accuracy)\n",
    "    4. [Class imbalance problem](#1.D.-Class-imbalance-problem)\n",
    "        1. [Add weights to the loss function](#1.D.a.-Add-weights-to-the-loss-function-(1-point)) **(1 point)**\n",
    "\n",
    "\n",
    "# 1. Multiclass pixel classification\n",
    "[Back to index](#Index)\n",
    "\n",
    "Throughout this lab, we will focus on applying neural networks to the characterization of the cell nuclei of histopathology tissue. This is a routine task in clinics, and it is generally done on color stained images. The most widely used stain is H&E (Hematoxylin and Eosin), that highlights the nuclei in dark color. \n",
    "\n",
    "Run the cell below to visualize the images that we will use for this.\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> If you click on <code>Options</code>, you can enable the <code>Joint Zoom</code> and zoom into different areas of the image to see how the pixels are classified.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8788a759a28c90eb779198d6aec0a8f",
     "grade": false,
     "grade_id": "cell-86ac774105d1677b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a250aaa4cc4c40db8f73be0c4c625abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "view = viewer([input_img, label_img], title=['Input Image', 'Grount Truth Classification'], subplots=(1,2), widgets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b2738f60a7007df4e298ec9ebfd36e8",
     "grade": false,
     "grade_id": "cell-201b9662c77b9855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see in the images above, in the label we have three color levels, $1)$ nuclei in brown, $2)$ nuclei in dark purple, and $3)$ background in bright purple. The goal is to automatically segment the image by assigning one of these three classes to every pixel of the input image. For this lab, using the above images as training dataset, we will train a fully connected neural network to classify pixels based on their three input values: red, green and blue (RGB). Then, we will test this trained network on a larger stained image.\n",
    "\n",
    "## 1.A. Data preparation\n",
    "### 1.A.a. Creating the input and target dataset\n",
    "[Back to index](#Index)\n",
    "\n",
    "First, we need to create our desired output classes. As we know, the label image consists of only three distinct RGB values, so we assign a number $(0$, $1$ or $2)$ to each one of these values and create an array that maps each pixel of the labeled image to one of these three classes. For this we use the [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function with the argument `return_inverse=True`, which returns the three unique RGB values as well as the mentioned array that maps each pixel to one of the classes.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> Since the input to our neural network will be a vector of RGB values, we flatten the labeled image into a vector as well, before assigning the pixels to their classes. For that, we use the <code>np.array</code> method <code>reshape</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce488d74e06c8e27a2ab3094fd9bd489",
     "grade": false,
     "grade_id": "cell-764f8c7853a1455a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three unique RGB values of the labeled image are:\n",
      "[[ 88  78 109]\n",
      " [159 134 142]\n",
      " [187 180 180]]\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "pixel_rgb_val, train_label_class_index = np.unique(label_img.reshape((-1, 3)), axis=0, return_inverse=True)\n",
    "print(f'The three unique RGB values of the labeled image are:\\n{pixel_rgb_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19ff2c66b401a196525558b8d78361b7",
     "grade": false,
     "grade_id": "cell-8170c5f2a0148d89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we create the PyTorch tensors `net_input` and `net_target` from our images. The input should be a vector of RGB values and the output should be a simple vector with the corresponding classes. Run the next cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "271f9fbc6c79f09880158098414d3f27",
     "grade": false,
     "grade_id": "cell-42dbb4d02ed5b11a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([22500, 3]), target size: torch.Size([22500])\n"
     ]
    }
   ],
   "source": [
    "# The input should be of type float\n",
    "net_input = torch.FloatTensor(input_img.reshape((-1, 3)))\n",
    "# The target class should be of type long\n",
    "net_target = torch.LongTensor(train_label_class_index)\n",
    "print(f'Input size: {net_input.shape}, target size: {net_target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fa823b12c0d39bcf7990f9498a2d553",
     "grade": false,
     "grade_id": "cell-87141478a4ebe508",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.A.b. Split the datasets into train and validation sets\n",
    "[Back to index](#Index)\n",
    "\n",
    "The size of our cell image is $(150 \\times 150)$ pixels and thus, we have a total of $150 \\times 150 = 22\\,500$ RGB pixels. In order to monitor the training process, we need to split our dataset into a training and a validation set, which is typically done in a ratio of $20%\\$ validation to $80\\%$ training. The difference between a test set, as we used it in the first notebook, and a validation set, is that the validation set helps us to monitor the performance of the model during the training process, while a test set is usually used to measure the performance of the final trained model. As such, a validation set is usually used to calculate the accuracy of the model on new data at each training step, which helps to detect [overfitting](https://www.investopedia.com/terms/o/overfitting.asp). For this we will again use the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from [`sklearn`](https://scikit-learn.org/stable/) as in the previous notebook, but here the generated test set will be our validation set. Run the cell below to create the train and validation sets.\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> As mentioned before, our <b>test set</b> will consist of one larger stained cell image that we loaded as <code>input_img_test</code> and <code>label_img_test</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86bf4e353ad6a1ae4cc2175c0a9bd03f",
     "grade": false,
     "grade_id": "cell-3ef3a300ee56efc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 18000 pixels and the validation set contains 4500 pixels.\n"
     ]
    }
   ],
   "source": [
    "# ratio between training and validation sets\n",
    "validation_ratio = 0.2\n",
    "\n",
    "splitting = train_test_split(net_input, net_target, test_size=validation_ratio, random_state=0)\n",
    "net_train_input, net_valid_input, net_train_target, net_valid_target = splitting\n",
    "print(f'The training set contains {len(net_train_input)} pixels and the validation set contains {len(net_valid_input)} pixels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab0059cda7a8b252f6728b039b7ab923",
     "grade": false,
     "grade_id": "cell-6c4c4409d41f5ba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.A.c. Creating the DataLoader\n",
    "[Back to index](#Index)\n",
    "\n",
    "In the [first part of the lab](./1_NN_Basics.ipynb), we used very few samples to train our networks and so we could simply use the entire dataset at once for the training. Now that we have a lot more samples, we need to train our model on small batches, called minibatches, of the entire dataset, otherwise there might not be enough memory available to handle the calculation of all the derivatives. \n",
    "\n",
    "To do this, we will create a `DataLoader` that serves 100 samples as a minibatch for each training step. \n",
    "\n",
    "PyTorch provides two data primitives: [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). They allow you to use pre-loaded datasets as well as your own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable* around the Dataset to enable easy access to the samples. \n",
    "\n",
    "In this exercise, we will see how to make a custom `Dataset` class. \n",
    "`torch.utils.data.Dataset` is an abstract class representing a dataset and its template is as follows:\n",
    "```python\n",
    "class Your_Custom_Dataset_Name(Dataset):\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.x = x        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "```\n",
    "\n",
    "Your custom dataset should inherit `Dataset` and override the following methods:\n",
    "\n",
    "* **`__len__`** so that `len(dataset)` returns the size of the dataset.\n",
    "* **`__getitem__`** to support the indexing such that `dataset[i]` can be used to get the $i^{\\mathrm{th}}$ sample of the dataset.\n",
    "\n",
    "If you want to see another example, you can check out the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class) on the Dataset class. \n",
    "\n",
    "_*\"An iterable is any Python object capable of returning its members one at a time, permitting it to be iterated over in a for-loop.\"_\n",
    "\n",
    "Run the cell below to define the custom `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c44f831c5cd3b468b8b7574fae173d63",
     "grade": false,
     "grade_id": "cell-a18f70de364ec2fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader:\n",
      "\tnumber of batches: 180, samples per batch: 100\n",
      "valid_loader:\n",
      "\tnumber of batches: 45, samples per batch: 100\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Use the dataset template for your own data! \n",
    "class CellDataset(Dataset):\n",
    "    # Initializes the dataset with our input data and labels\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    # Returns the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Returns the item at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        # We return the input and target values\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Train in batch mode\n",
    "batch_size = 100\n",
    "    \n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset=CellDataset(net_train_input, net_train_target), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=CellDataset(net_valid_input, net_valid_target), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'train_loader:\\n\\tnumber of batches: {len(train_loader)}, samples per batch: {len(iter(train_loader).__next__()[0])}')\n",
    "print(f'valid_loader:\\n\\tnumber of batches: {len(valid_loader)}, samples per batch: {len(iter(valid_loader).__next__()[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5af12aecc3a47d200bdeb391ba923097",
     "grade": false,
     "grade_id": "cell-56d690a3154f8b5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.B. Softmax and cross-entropy loss for multiclass classification\n",
    "[Back to index](#Index)\n",
    "\n",
    "To solve a classification problem using a neural network, at some point, one needs to map a network output $x$ (which is unbounded) into the value between $0$ and $1$ so that it can be interpreted as a probability for each class $i\\in \\{1, \\ldots, C\\}$. For the binary classification problem in the previous notebook, we used the sigmoid function to map the single output value to the range $[0,1]$. Now, for the multiclass classification problem, we need a function that can act on multiple values at once, for which, we will use the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function:\n",
    "\n",
    "$$\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^Ce^{x_j}}$$\n",
    "where $C$ is the total number of classes (in our case 3).\n",
    "\n",
    "In the deep learning community, $x$ is typically referred to as `logits`, which is the vector of raw predictions that a classification model generates. For the multiclass classification problem, $x$ is normalized by using the softmax function, which generates a vector of probabilities corresponding to each of the classes, and $\\Sigma_{i=0}^{N} \\sigma(x)_i  = 1$, that is, the sum of all the elements of this probability vector is $1$. Then, we can measure the performance of a classification model using the `cross_entropy` loss,  which increases as the predicted probability diverges from the actual label. \n",
    "\n",
    "In the next sections we will go through a series of simple toy examples to calrify these concepts.\n",
    "\n",
    "### 1.B.a. Softmax in NumPy (1 point)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Let's start by implementing the mentioned softmax function. In the cell below, **for 1 point** implement the softmax function from scratch using only NumPy operators.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Important:</b> Do not use for loops!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf5f7b7e03c400b7d2be932d9a8ae170",
     "grade": false,
     "grade_id": "cell-0a60482b7d7439e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculates the softmax of an input vector x\n",
    "def softmax(x):\n",
    "    out = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    out = np.exp(x)/np.exp(x).sum()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c754759bdd1a8e8e8f4df41546e2b0ed",
     "grade": false,
     "grade_id": "cell-f5176d243f6c6814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As usual, run the next cell for a quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fbb825931f105c2fbdbb312594972c9",
     "grade": true,
     "grade_id": "cell-0460b5eb0e50162d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your softmax output for x=[2.  1.  0.1] is [0.659  0.2424 0.0986]\n",
      "Nice, you passed the sanity check. This does guarantee the points though!\n"
     ]
    }
   ],
   "source": [
    "# Input vector\n",
    "x = np.array([2.0, 1.0 ,0.1])\n",
    "# Calculate softmax\n",
    "outputs = softmax(x)\n",
    "print(f'Your softmax output for x={x} is {np.round(outputs, 4)}')\n",
    "# Check that the sum is 1\n",
    "assert np.round(sum(outputs), 4) == 1, f'The sum of the softmax output should always be 1, yours is {sum(outputs):.4f}'\n",
    "print('Nice, you passed the sanity check. This does guarantee the points though!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e268bb2b9f21dc708bf001aa9371f31b",
     "grade": false,
     "grade_id": "cell-c3a2efe14cde57f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.b. Softmax in PyTorch\n",
    "[Back to index](#Index)\n",
    "\n",
    "As you may already have guessed, we can also use PyTorch to calculate the softmax. For this we can make use of the function [`torch.nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html), which we will now use to test your NumPy implementation. Run the cell below to check that your `softmax` function produces the same result as the one from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c4b812a94bca7132371197a211a08e0",
     "grade": false,
     "grade_id": "cell-12526e65a87e2d1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PyTorch softmax output is tensor([0.6590, 0.2424, 0.0986])\n",
      "Great! Your sofmax produced the same result as PyTorch.\n"
     ]
    }
   ],
   "source": [
    "# Create the input tensor\n",
    "x_tc = torch.tensor([2.0, 1.0 ,0.1])\n",
    "# Calculate the softmax with PyTorch\n",
    "softmax_tc = torch.nn.Softmax(dim=0)\n",
    "# torch.nn.Softmax() returns a function which we can evaluate on any input\n",
    "outputs_tc = softmax_tc(x_tc)\n",
    "print(f'The PyTorch softmax output is {outputs_tc}')\n",
    "# Check that the outputs are equal\n",
    "np.testing.assert_array_almost_equal(outputs_tc, outputs, err_msg = 'Your NumPy implementation does not match the PyTorch output!')\n",
    "print('Great! Your sofmax produced the same result as PyTorch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13546782a4c7e33f81a5dbde5ad4a94e",
     "grade": false,
     "grade_id": "cell-ce1db3a666b11e65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.c. Cross-entropy in NumPy (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Cross-entropy is commonly used in classification tasks both in traditional machine learning and deep learning. It is defined as\n",
    "\n",
    "$$\\operatorname{H}(y, t) = -\\sum_{i=1}^Ct_i\\log(y_i)$$\n",
    "with $y$ the prediction and $t$ the target vector.\n",
    "\n",
    "and it measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy increases as the predicted probability diverges from the actual label. So predicting a probability of $y = [0.01, 0.00, 0.99]$ when the actual observation label is $t = [1,0,0]$ would be bad and result in a high loss value. A perfect prediction on the other hand would result in a cross-entropy loss of 0.\n",
    "\n",
    "In the next cell, **for 1 point**, implement the `cross_entropy` function that calculates the cross entropy loss as defined in the formula above from scratch, only using NumPy operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af0be3e44242816299576b084d6b307b",
     "grade": false,
     "grade_id": "cell-5baf9049b77dc840",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculates the cross-entropy loss\n",
    "def cross_entropy(predicted, target):\n",
    "    loss = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    loss = -np.sum(target * np.log(predicted))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74de9e9be6fc438c8d93a8d40de2d299",
     "grade": false,
     "grade_id": "cell-13f79525d64b37e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell for a simple sanity check that checks your function for a very good and a very bad example prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f24e6a42e7e56fc694fd7ec9860565",
     "grade": true,
     "grade_id": "cell-e1442867cbccbfe3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good, your cross_entropy function provides the correct result for these simple test cases. Make sure to double check it anyway!\n"
     ]
    }
   ],
   "source": [
    "# Check that the loss of a very good prediction is 0\n",
    "good_prediction = np.round(cross_entropy(np.array([1, 1e-5, 1e-5]), np.array([1, 0, 0])), 4)\n",
    "assert good_prediction == 0, f'The cross-entropy loss of a very good prediction should be 0. Your loss is {good_prediction}.'\n",
    "# Check that the loss of a very bad prediction is large\n",
    "bad_prediction = np.round(cross_entropy(np.array([1e-5, 1, 1]), np.array([1, 0, 0])), 4)\n",
    "assert bad_prediction == 11.5129, f'The cross-entropy loss of a very bad prediction should be large (11.5129). Your loss is {bad_prediction}.'\n",
    "print('Good, your cross_entropy function provides the correct result for these simple test cases. Make sure to double check it anyway!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52e266e637c3b8ad07d4e0455cb539bc",
     "grade": false,
     "grade_id": "cell-6cfc3c57f88a127e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, you can use the next cell to try different prediction-target combinations. This might help you in the upcoming MCQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54e239feecd87fa0fc9cf8444da315a3",
     "grade": false,
     "grade_id": "cell-7e25abf439e9eda2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245 2.3025850929940455 0.4170300162778335\n"
     ]
    }
   ],
   "source": [
    "# Remember that target is one-hot encoded (binary indicator of each class)\n",
    "# And the predictions vector should sum to 1\n",
    "# YOUR CODE HERE\n",
    "error_1 = cross_entropy([0.7, 0.2, 0.1], [1, 0, 0])\n",
    "error_2 = cross_entropy([0.1, 0.3, 0.6], [1, 0, 0])\n",
    "error_3 = cross_entropy(softmax([2.0, 1.0 ,0.1]), [1, 0, 0])\n",
    "print(error_1, error_2, error_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "354af9a7e6bdf3825a57dcadf62c20b5",
     "grade": false,
     "grade_id": "cell-9fdf6a7d1c8f4cab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multiple Choice Question\n",
    "\n",
    "Now at this point you may be asking yourself, does this make sense? If you looked closely at the cross-entropy values, there must be some intuitive order among the errors given the predictions. That said, for **1 point** answer the following. \n",
    "\n",
    "* Q1. Assume we calculate the cross-entropy errors `error_1 = cross_entropy([0.7, 0.2, 0.1], [1, 0, 0])`, `error_2 = cross_entropy([0.1, 0.3, 0.6], [1, 0, 0])`, and `error_3 = cross_entropy(softmax([2.0, 1.0 ,0.1]), [1, 0, 0])`.<br>What will be the order of the errors, from low to high? \n",
    "\n",
    "1. `error_1` < `error_2` < `error_3`\n",
    "2. `error_1` < `error_3` < `error_2`\n",
    "3. `error_2` < `error_1` < `error_3`\n",
    "4. `error_2` < `error_3` < `error_1`\n",
    "5. `error_3` < `error_1` < `error_2`\n",
    "6. `error_3` < `error_2` < `error_1`\n",
    "\n",
    "Modify the variable answer in the following cell to reflect your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "293c79c17aa9fbcd103b2cf80a13e22b",
     "grade": false,
     "grade_id": "cell-ff1237d6e7172c44",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assign your answer to this variable\n",
    "answer = 2\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6807f3e7490b86738fa281dd3d152741",
     "grade": true,
     "grade_id": "cell-aeb5fc657f4a288f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert answer in [1, 2, 3, 4, 5, 6], 'Choose one of 1, 2, 3, 4, 5 or 6.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "597d056d8b301229e69d2dfa7ba829a6",
     "grade": false,
     "grade_id": "cell-8854972520890683",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.d. Cross-entropy in PyTorch\n",
    "[Back to index](#Index)\n",
    "\n",
    "Of course PyTorch also provides a cross-entropy loss function: [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). However, this function combines the softmax together with the original cross-entropy function, which are averaged across observations for each minibatch of size $N$, so that the input to the PyTorch cross-entropy function is not the output of the softmax, but simply the `logits`, which is the raw unbounded net output:\n",
    "\n",
    "$$\\text{CrossEntropyLoss}()= \\frac{\\sum^{N}_{n=1} \\mathrm{H}(x^{(n)}, i^{(n)})}{N},$$\n",
    "$$\\text{where}\\quad\\mathrm{H}(x, i) = -\\log\\left(\\frac{e^{x_i}}{\\sum_{j=1}^C e^{x_j}}\\right)$$\n",
    "with $N$ the number of samples per minibatch, $x$ the prediction and $i$ the target class index.\n",
    "\n",
    "In addition, the target input of the [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) is the **target class index** instead of the one-hot encoded vector used before.\n",
    "\n",
    "Let's check now if the results obtained from PyTorch match the ones from our own NumPy function. Run the cell below to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "259628b64c738cbdcc238e4128bfa094",
     "grade": false,
     "grade_id": "cell-4577942a3f8d66d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your cross-entropy output is 0.4170.\n",
      "The PyTorch cross-entropy output is 0.4170.\n",
      "Nice! Your cross-entropy produced the same result as PyTorch.\n"
     ]
    }
   ],
   "source": [
    "# NumPy input and target vector\n",
    "x = np.array([2.0, 1.0 ,0.1])\n",
    "target = np.array([1, 0, 0])\n",
    "# Calculate softmax\n",
    "x_sm = softmax(x)\n",
    "# Calculate NumPy cross-entropy\n",
    "outputs = cross_entropy(x_sm, target)\n",
    "\n",
    "# Create the input and target tensor (needs to be 2-dimensional)\n",
    "x_tc = torch.tensor([x])\n",
    "# The class index 0 corresponds to [1, 0, 0]\n",
    "target_tc = torch.tensor([0])\n",
    "# Get the PyTorch cross entropy function and evaluate it\n",
    "cross_entropy_loss_tc = torch.nn.CrossEntropyLoss()\n",
    "outputs_tc = cross_entropy_loss_tc(x_tc, target_tc)\n",
    "\n",
    "print(f'Your cross-entropy output is {outputs:.4f}.')\n",
    "print(f'The PyTorch cross-entropy output is {outputs_tc:.4f}.')\n",
    "# Check that the outputs are equal\n",
    "np.testing.assert_array_almost_equal(outputs_tc, outputs)\n",
    "print('Nice! Your cross-entropy produced the same result as PyTorch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bff497d6e59df239b6e2692d416a5660",
     "grade": false,
     "grade_id": "cell-24f59945d8af5157",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.C. Build a model and train it with PyTorch\n",
    "### 1.C.a. Define the model (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "To begin this section, and **for 2 points, complete the function `build_model`** that defines a model with three linear layers ([`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) (input, hidden and output) of which the first two also have a ReLU activation function ([`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)). The output layer does not have an activation function since we want the output to be the `logits`, which we can then use as an input to the cross-entropy loss function. Use the [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) module to build the model and run the cell below the next one to check that your answers are valid. Note that `build_model` has no input parameters, and the described model as output.\n",
    "\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "**Note:** If you don't remember how to build a model using [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html), look at section 1.B.c. of the [previous notebook](./1_NN_Basics.ipynb) again.  \n",
    "</div> \n",
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "**Important:** We want the hidden layer to have 64 neurons. You need to choose the appropriate input and output size corresponding to our data! You will need to hardcode it in the function, as it has no input parameters.\n",
    "</div> \n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "**Hint:** Our goal is to classify an RGB pixel into one of three classes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71f5fc4487ebdd1b5c8e43cdea800f00",
     "grade": false,
     "grade_id": "cell-46deb6e98adaac42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters in your model is: 4611\n",
      "\n",
      "This is your model:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build a model with the given hyper-parameters\n",
    "model = None\n",
    "\n",
    "def build_model():\n",
    "    # Hard-code hyper-parameters (change values)\n",
    "    input_size = None\n",
    "    hidden_size = 64\n",
    "    output_size = None\n",
    "    # Initialize model variable (to redefine)\n",
    "    model = None\n",
    "    # YOUR CODE HERE\n",
    "    input_size = 3\n",
    "    output_size = 3\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, output_size)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Declare the model\n",
    "model = build_model()\n",
    "print(f'The total number of parameters in your model is: {sum([np.prod(list(pnb.size())) for pnb in model.parameters()])}\\n')\n",
    "print(f'This is your model:\\n{model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f63027d6dc1bac53ded4a6094912d9c3",
     "grade": false,
     "grade_id": "cell-ac0b6188d5857867",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell to perform an elaborate test on your model. It checks everything from the number and type of layers up to the input and output sizes of the layers, so that you can be sure that you have a correct model to continue the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "125e34c9827e2ed4c6bbe0582093d183",
     "grade": true,
     "grade_id": "cell-40ac96f5efdd3c35",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done! The model seems to be correct.\n"
     ]
    }
   ],
   "source": [
    "# First we redeclare the model, to be sure\n",
    "model = build_model()\n",
    "# Check that the model has 5 layers\n",
    "assert len(model._modules.items()) == 5, f'The model should have a total of 5 layers: 3 linear and 2 ReLU. Yours currently has {len(model._modules.items())} layers.'\n",
    "# Check that the layer types are correct\n",
    "for i, layer in enumerate(model._modules.items()):\n",
    "    current_layer = torch.nn.modules.linear.Linear if i % 2 == 0 else torch.nn.modules.activation.ReLU\n",
    "    assert type(layer[1]) == current_layer, f'Layer {i} should be of type {current_layer}, not {type(layer[1])}.'\n",
    "# Check that the input and output size of the layers is correct and that torch.nn.Sequential was used\n",
    "for i, module in enumerate(model.modules()):\n",
    "    if i == 0:\n",
    "        assert type(module) == torch.nn.modules.container.Sequential, 'You should use torch.nn.Sequential to build your model!'\n",
    "    if i == 1:\n",
    "        assert module.in_features == 3, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that we are trying to classify RGB pixels.'\n",
    "        assert module.out_features == 64, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the hidden layer should be of size 64.'\n",
    "    if i == 3:\n",
    "        assert module.in_features == 64, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that the hidden layer should be of size 64.'\n",
    "        assert module.out_features == 64, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the hidden layer should be of size 64.'\n",
    "    if i == 5:\n",
    "        assert module.in_features == 64, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that the hidden layer should be of size 64.'\n",
    "        assert module.out_features == 3, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the output size should be equal to the number of classes we have.'\n",
    "print('Well done! The model seems to be correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee95cd572256aa2e97f0e4d89c912511",
     "grade": false,
     "grade_id": "cell-bf59ac1f0906bb29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.C.b. Complete the training pipeline (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Now that we have the desired model, we need to implement the training workflow that optimizes our model. The basic structure has already been created so you only need to implement some of the important steps. In the next cell, **for 2 points**, complete the function `train` that takes as input parameters:\n",
    "* `model` : A PyTorch model, e.g. the output of `build_model`,\n",
    "* `train_loader` : The training `DataLoader` that we defined in section [1.A.c.](#1.A.c.-Creating-the-DataLoader),\n",
    "* `valid_loader` : The validation `DataLoader` that we defined in section [1.A.c.](#1.A.c.-Creating-the-DataLoader),\n",
    "* `optimizer` : An optimization function from the [`torch.optim`](https://pytorch.org/docs/stable/optim.html) module. Here we use [`torch.optim.Adam`](https://pytorch.org/docs/master/generated/torch.optim.Adam.html),\n",
    "* `loss_fn` : A loss function of the form `loss_fn(predicted, target)`. Here we use [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html),\n",
    "* `max_iter` : The maximum number of training batches to use (defaults to 50),\n",
    "* `save_model` : Tells the function if it should save the best model during training. Defaults to `True`, and \n",
    "* `save_model_name` : The name under which the model will be saved, defaults to `'best_model'`.\n",
    "\n",
    "and returns the evolution (history) of the training and validation loss as well as the highest validation accuracy achieved.\n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "    <b>Hint:</b> If in doubt, you can take inspiration from the training workflows we defined in the first part of the notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aac11a991bb8643aaee925c05008175c",
     "grade": false,
     "grade_id": "cell-533966e8c5d0eed6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, loss_fn, max_iter=50, save_model_name='best_model', save_model=True):\n",
    "    # Some initializations\n",
    "    lowest_loss = np.inf\n",
    "    highest_accuracy = 0.\n",
    "    train_loss, valid_loss, train_accuracy, valid_accuracy = [], [], [], []\n",
    "    train_batch_loss, correct, lowest_iteration = 0, 0, 0\n",
    "    \n",
    "    # Tell the model that we are in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate through the training minibatches\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Extract the train and target minibatches\n",
    "        x_i, y_i = data[0], data[1]\n",
    "        \n",
    "        # Get the current loss value and training predictions (forward pass)\n",
    "        # Use the same variable names as the ones initialized next\n",
    "        loss, y_hat_i = None, None        \n",
    "        # YOUR CODE HERE\n",
    "        y_hat_i = model.forward(x_i)\n",
    "        loss = loss_fn(y_hat_i, y_i)\n",
    "        # Perform the backward pass (remember to first reset the gradients)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        loss.backward()\n",
    "        # Perform one training step (optimization)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate the average loss\n",
    "        train_batch_loss = float(loss) / len(y_i)\n",
    "        train_loss.append(train_batch_loss)\n",
    "\n",
    "        # Calculate the validation loss and accuracy\n",
    "        with torch.no_grad():\n",
    "            # Tell the model that we are in evaluation mode\n",
    "            model.eval()\n",
    "            valid_batch_loss, correct = 0, 0\n",
    "            # Iterate through the validation minibatches\n",
    "            for x_i, y_i in valid_loader:\n",
    "                \n",
    "                # Get the current validation loss value\n",
    "                loss = None\n",
    "                # YOUR CODE HERE\n",
    "                y_hat_i = model.forward(x_i)\n",
    "                loss = loss_fn(y_hat_i, y_i)\n",
    "                # Accumulate the loss\n",
    "                valid_batch_loss += float(loss)\n",
    "                # Get the predicted class (max probability)\n",
    "                _, predicted = torch.max(y_hat_i.data, 1)\n",
    "                # Accumulate accuracy\n",
    "                correct += (predicted == y_i).sum().item()\n",
    "\n",
    "            # Calculate average loss and accuracy\n",
    "            valid_batch_loss = valid_batch_loss / len(valid_loader)\n",
    "            valid_batch_accuracy = 100 * correct / (len(valid_loader) * batch_size)\n",
    "            valid_accuracy.append(valid_batch_accuracy)\n",
    "            valid_loss.append(valid_batch_loss)\n",
    "            \n",
    "        # Save the model if we had an improvement\n",
    "        if valid_batch_loss <= lowest_loss:\n",
    "            lowest_loss = valid_batch_loss\n",
    "            lowest_iteration = i        \n",
    "            highest_accuracy = valid_batch_accuracy\n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), save_model_name + '.pt')\n",
    "        \n",
    "        # Print current model state\n",
    "        print(f'Iteration {i+1:2}: train loss={train_batch_loss:8.4f}  valid_loss={valid_batch_loss:8.4f}  valid_acc={valid_batch_accuracy:5.2f} %  best accuracy(@iter{lowest_iteration:2})={highest_accuracy:5.2f}')\n",
    "        # Stop after 50 iterations\n",
    "        if (i+1) % max_iter == 0:\n",
    "            break \n",
    "        \n",
    "            \n",
    "    return train_loss, valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "217878b3d34ed382d4c1dd66f98b8f84",
     "grade": false,
     "grade_id": "cell-afb2d5657dd7bc24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's try to train your model! Run the next cell to perform a training using only 50 of the 180 training minibatches. If you implemented everything correctly, you should see how both the training and validation loss decrease and the accuracy increases as the training advances.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b>Warning:</b> We limit the number of minibatches here so that you don't have to wait too long for\n",
    "    the training to finish since Noto does not provide a lot of computing power. Still, the training might take a few minutes to run... \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dde35515b4ce33033f26fcb1c54ee3b6",
     "grade": true,
     "grade_id": "cell-a06dc949523babcc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1: train loss=  0.1110  valid_loss= 42.0004  valid_acc=39.44 %  best accuracy(@iter 0)=39.44\n",
      "Iteration  2: train loss=  0.4353  valid_loss= 25.2387  valid_acc=39.44 %  best accuracy(@iter 1)=39.44\n",
      "Iteration  3: train loss=  0.2684  valid_loss= 10.0630  valid_acc= 1.98 %  best accuracy(@iter 2)= 1.98\n",
      "Iteration  4: train loss=  0.1019  valid_loss= 10.5725  valid_acc=58.58 %  best accuracy(@iter 2)= 1.98\n",
      "Iteration  5: train loss=  0.1130  valid_loss= 13.3862  valid_acc=58.58 %  best accuracy(@iter 2)= 1.98\n",
      "Iteration  6: train loss=  0.1308  valid_loss= 12.2141  valid_acc=58.58 %  best accuracy(@iter 2)= 1.98\n",
      "Iteration  7: train loss=  0.1202  valid_loss=  8.6166  valid_acc=58.58 %  best accuracy(@iter 6)=58.58\n",
      "Iteration  8: train loss=  0.0675  valid_loss=  3.6889  valid_acc=58.58 %  best accuracy(@iter 7)=58.58\n",
      "Iteration  9: train loss=  0.0342  valid_loss=  3.3686  valid_acc=39.44 %  best accuracy(@iter 8)=39.44\n",
      "Iteration 10: train loss=  0.0368  valid_loss=  4.4317  valid_acc=39.44 %  best accuracy(@iter 8)=39.44\n",
      "Iteration 11: train loss=  0.0516  valid_loss=  1.0475  valid_acc=41.04 %  best accuracy(@iter10)=41.04\n",
      "Iteration 12: train loss=  0.0120  valid_loss=  2.7094  valid_acc=58.58 %  best accuracy(@iter10)=41.04\n",
      "Iteration 13: train loss=  0.0260  valid_loss=  3.7713  valid_acc=58.58 %  best accuracy(@iter10)=41.04\n",
      "Iteration 14: train loss=  0.0440  valid_loss=  3.9138  valid_acc=58.58 %  best accuracy(@iter10)=41.04\n",
      "Iteration 15: train loss=  0.0385  valid_loss=  3.2424  valid_acc=58.58 %  best accuracy(@iter10)=41.04\n",
      "Iteration 16: train loss=  0.0311  valid_loss=  2.1071  valid_acc=58.87 %  best accuracy(@iter10)=41.04\n",
      "Iteration 17: train loss=  0.0187  valid_loss=  0.8729  valid_acc=68.51 %  best accuracy(@iter16)=68.51\n",
      "Iteration 18: train loss=  0.0091  valid_loss=  0.9466  valid_acc=39.60 %  best accuracy(@iter16)=68.51\n",
      "Iteration 19: train loss=  0.0145  valid_loss=  1.4914  valid_acc=39.44 %  best accuracy(@iter16)=68.51\n",
      "Iteration 20: train loss=  0.0148  valid_loss=  1.0308  valid_acc=39.38 %  best accuracy(@iter16)=68.51\n",
      "Iteration 21: train loss=  0.0097  valid_loss=  0.4807  valid_acc=84.22 %  best accuracy(@iter20)=84.22\n",
      "Iteration 22: train loss=  0.0045  valid_loss=  0.5371  valid_acc=76.67 %  best accuracy(@iter20)=84.22\n",
      "Iteration 23: train loss=  0.0076  valid_loss=  0.7334  valid_acc=70.07 %  best accuracy(@iter20)=84.22\n",
      "Iteration 24: train loss=  0.0076  valid_loss=  0.7819  valid_acc=69.09 %  best accuracy(@iter20)=84.22\n",
      "Iteration 25: train loss=  0.0091  valid_loss=  0.6305  valid_acc=73.31 %  best accuracy(@iter20)=84.22\n",
      "Iteration 26: train loss=  0.0066  valid_loss=  0.4367  valid_acc=82.27 %  best accuracy(@iter25)=82.27\n",
      "Iteration 27: train loss=  0.0057  valid_loss=  0.3869  valid_acc=85.24 %  best accuracy(@iter26)=85.24\n",
      "Iteration 28: train loss=  0.0053  valid_loss=  0.4719  valid_acc=80.82 %  best accuracy(@iter26)=85.24\n",
      "Iteration 29: train loss=  0.0044  valid_loss=  0.4846  valid_acc=79.82 %  best accuracy(@iter26)=85.24\n",
      "Iteration 30: train loss=  0.0053  valid_loss=  0.3982  valid_acc=83.84 %  best accuracy(@iter26)=85.24\n",
      "Iteration 31: train loss=  0.0042  valid_loss=  0.3415  valid_acc=86.04 %  best accuracy(@iter30)=86.04\n",
      "Iteration 32: train loss=  0.0040  valid_loss=  0.3399  valid_acc=86.47 %  best accuracy(@iter31)=86.47\n",
      "Iteration 33: train loss=  0.0034  valid_loss=  0.3588  valid_acc=85.16 %  best accuracy(@iter31)=86.47\n",
      "Iteration 34: train loss=  0.0046  valid_loss=  0.3550  valid_acc=85.40 %  best accuracy(@iter31)=86.47\n",
      "Iteration 35: train loss=  0.0029  valid_loss=  0.3401  valid_acc=86.64 %  best accuracy(@iter31)=86.47\n",
      "Iteration 36: train loss=  0.0041  valid_loss=  0.3253  valid_acc=87.13 %  best accuracy(@iter35)=87.13\n",
      "Iteration 37: train loss=  0.0028  valid_loss=  0.3273  valid_acc=87.58 %  best accuracy(@iter35)=87.13\n",
      "Iteration 38: train loss=  0.0035  valid_loss=  0.3369  valid_acc=87.16 %  best accuracy(@iter35)=87.13\n",
      "Iteration 39: train loss=  0.0039  valid_loss=  0.3359  valid_acc=87.11 %  best accuracy(@iter35)=87.13\n",
      "Iteration 40: train loss=  0.0022  valid_loss=  0.3259  valid_acc=87.62 %  best accuracy(@iter35)=87.13\n",
      "Iteration 41: train loss=  0.0033  valid_loss=  0.3290  valid_acc=87.11 %  best accuracy(@iter35)=87.13\n",
      "Iteration 42: train loss=  0.0029  valid_loss=  0.3435  valid_acc=86.33 %  best accuracy(@iter35)=87.13\n",
      "Iteration 43: train loss=  0.0037  valid_loss=  0.3477  valid_acc=85.67 %  best accuracy(@iter35)=87.13\n",
      "Iteration 44: train loss=  0.0033  valid_loss=  0.3342  valid_acc=86.53 %  best accuracy(@iter35)=87.13\n",
      "Iteration 45: train loss=  0.0032  valid_loss=  0.3158  valid_acc=87.31 %  best accuracy(@iter44)=87.31\n",
      "Iteration 46: train loss=  0.0033  valid_loss=  0.3003  valid_acc=87.78 %  best accuracy(@iter45)=87.78\n",
      "Iteration 47: train loss=  0.0032  valid_loss=  0.2967  valid_acc=88.02 %  best accuracy(@iter46)=88.02\n",
      "Iteration 48: train loss=  0.0041  valid_loss=  0.3019  valid_acc=87.33 %  best accuracy(@iter46)=88.02\n",
      "Iteration 49: train loss=  0.0037  valid_loss=  0.3002  valid_acc=87.40 %  best accuracy(@iter46)=88.02\n",
      "Iteration 50: train loss=  0.0027  valid_loss=  0.2917  valid_acc=87.87 %  best accuracy(@iter49)=87.87\n",
      "\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "# Redefine model\n",
    "model = build_model()\n",
    "\n",
    "# Training related hyper-parameters\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# set the loss function loss_fn\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# set the Adam optimizer with the given learning rate \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Get DataLoaders\n",
    "train_loader = DataLoader(dataset=CellDataset(net_train_input, net_train_target), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=CellDataset(net_valid_input, net_valid_target), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss, valid_loss, valid_acc = train(model, train_loader, valid_loader, optimizer, loss_fn, max_iter=50, save_model_name='best_model_without_weights')\n",
    "print('\\nFinished training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8c2d8fecb581ce1927b4b1335f38235",
     "grade": false,
     "grade_id": "cell-346dc7456c04020a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to see how the training and validation loss evolved during training. If you implemented everything correctly, you should be able to see how the validation loss starts off relatively high, but converges to 0 as the training iterations increase. The training loss already starts very small and stays small during the entire training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d739c4bab4cd40a9ad93bea2b4c245c8",
     "grade": true,
     "grade_id": "cell-de983118a48c0147",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0543db43fcc46f2aa4f87e7d6f42c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize = (9, 6))\n",
    "\n",
    "# Plot loss\n",
    "ax_loss = plt.gca()\n",
    "ax_loss.set_xlabel('Iterations')\n",
    "ax_loss.set_ylabel('Loss')\n",
    "plt.title('Training and Validation Loss and Accuracy History')\n",
    "p1 = ax_loss.plot(train_loss, 'y', label = 'Train Loss')\n",
    "p2 = ax_loss.plot(valid_loss, 'r', label = 'Val Loss')\n",
    "\n",
    "# Get twin axis and plot accuracy\n",
    "ax_acc = ax_loss.twinx()  \n",
    "ax_acc.set_ylabel('Accuracy') \n",
    "p3 = ax_acc.plot(valid_acc, 'k', label = 'Val Accuracy')\n",
    "legends = [l.get_label() for l in p1+p2+p3]\n",
    "plt.grid()\n",
    "ax_acc.legend(p1+p2+p3, legends)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9da1fc9d4e0bf7e49ea22a48bc6ee97",
     "grade": false,
     "grade_id": "cell-ecfb20a7a1a94caa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.C.c Test the model on a test image\n",
    "[Back to index](#Index)\n",
    "\n",
    "Now that the training is complete, we can test our model on a test image. Run the next cell to view the test image and the target image that we are trying to obtain.\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> If you click on <code>Options</code>, you can enable the <code>Joint Zoom</code> and zoom into different areas of the image to see how the pixels are classified.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fe4940b36ff7fce3c37238beb0fca0e",
     "grade": false,
     "grade_id": "cell-00124ba2ae7f3090",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071d92303b9642f7bdcc7a3475fb9ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "view = viewer([input_img_test, label_img_test], titles=['Test Image', 'Ground Truth'], subplots=(1,2), widgets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f69e8293ce7381f48245358945d10339",
     "grade": false,
     "grade_id": "cell-5be9b342520d0204",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, we will load the best-performing model from the disk using [`torch.load`](https://pytorch.org/docs/stable/generated/torch.load.html) together with [`load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict), since we saved it during training. Run the next cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74fbd117910ff57bbd8dca6fb990fdcd",
     "grade": false,
     "grade_id": "cell-b0ada0529f0b09a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_model_without_weights.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a71703c742e04bb93e5cbfc31a51830e",
     "grade": false,
     "grade_id": "cell-a1e3563a61adcf19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we convert the input test image to a PyTorch tensor and extract the correct classes from the target test image as we did for the training and test validation sets. Run the next cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02a10cb0e33cc94f92095bb6856c11f3",
     "grade": false,
     "grade_id": "cell-48a4133d70ebad4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three unique RGB values of the labeled image are:\n",
      "[[ 88  78 109]\n",
      " [159 134 142]\n",
      " [187 180 180]]\n"
     ]
    }
   ],
   "source": [
    "# Create test labels (with the correct RGB calues and the indexes that should have these values)\n",
    "pixel_rgb_val_test, test_label_class_index = np.unique(label_img_test.reshape((-1, 3)), axis=0, return_inverse=True)\n",
    "# Flatten the input and make it of type float\n",
    "net_input_test = torch.FloatTensor(input_img_test.reshape((-1, 3)))\n",
    "# Make the target class a tensor of type long\n",
    "net_target_test = torch.LongTensor(test_label_class_index)\n",
    "print(f'The three unique RGB values of the labeled image are:\\n{pixel_rgb_val_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33489bf4eee7398a7fe12ac8ffe28152",
     "grade": false,
     "grade_id": "cell-1836a642f4da92b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can simply provide the test image as an input to our model and compare it's output with the test target. Run the next cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c219b9a80035a27c5ae30fd27f990990",
     "grade": false,
     "grade_id": "cell-1efa253352982d1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.94 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1b49937b2d4a83b8d609f5dcb7a56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1aa7e9cbc64ae5adb84cbfd4022ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tell the model that we are in evaluation mode\n",
    "model.eval()\n",
    "# Generate prediciton\n",
    "y_hat = model(net_input_test)\n",
    "# Extract the corresponding classes from the one-hot encoded output vecotor\n",
    "_, predicted = torch.max(y_hat.data, 1)\n",
    "# Calculate the accuracy\n",
    "correct = (predicted == net_target_test).sum().item()\n",
    "test_accuracy = correct / len(net_target_test) * 100\n",
    "print(f'Test accuracy: {test_accuracy:.2f} %\\n')\n",
    "# Create the model output image\n",
    "predicted_img = pixel_rgb_val_test[predicted.data.cpu().numpy()].reshape(input_img_test.shape)\n",
    "# Display\n",
    "plt.close('all')\n",
    "view = viewer([input_img_test, predicted_img, label_img_test], title=['Test image', 'Model output', 'Correct classification'], subplots=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51f0b9f0ca36c1428710afd8195487e8",
     "grade": false,
     "grade_id": "cell-b219cc26487182ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** It was actually expected that one of the three classes is very badly classified! Check the next section to find out why and how to compensate for that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acb4a031df734bc1ea595034c24e4bee",
     "grade": false,
     "grade_id": "cell-37978d2dcf426d79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.C.d. Classwise accuracy\n",
    "[Back to index](#Index)\n",
    "\n",
    "As you saw above, the test accuracy for the entire dataset is fairly okay ($\\sim 80\\%$). However, the output results do not seem to classify correctly one of the classes. Why does this happen? To investigate this, let's first look into the classwise accuracy, instead of the total accuarcy, meaning that we calculate the accuracy of each class separately. Run the cell below to see how the model performs on each one of the three classes.\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> The accuracy changes each time you re-train your model because the dataset is shuffled again each time. If you want to see a good improvement in the next section, re-run the training until you have a very low accuracy for class 0. Normally this is not at all what we want from a model, but here it is preferable for illustration purposes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbe1b2c79f271c1d48b6f29aca8b217c",
     "grade": false,
     "grade_id": "cell-8f23232a7cf875ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (classwise)\n",
      "class 0: 77.23 % class 1: 59.39 % class 2: 92.65 %\n"
     ]
    }
   ],
   "source": [
    "cls0_acc_wo = 100 * np.logical_and((predicted == 0).data.cpu(), (net_target_test == 0).data.cpu()).sum().item()/(net_target_test == 0).sum().item()\n",
    "cls1_acc_wo = 100 * np.logical_and((predicted == 1).data.cpu(), (net_target_test == 1).data.cpu()).sum().item()/(net_target_test == 1).sum().item()\n",
    "cls2_acc_wo = 100 * np.logical_and((predicted == 2).data.cpu(), (net_target_test == 2).data.cpu()).sum().item()/(net_target_test == 2).sum().item()\n",
    "print('Test accuracy (classwise)\\nclass 0: %.2f %% class 1: %.2f %% class 2: %.2f %%' % (cls0_acc_wo,cls1_acc_wo,cls2_acc_wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fb463b635395d62c11063b0b2b9720c",
     "grade": false,
     "grade_id": "cell-31ae974972879de7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you see, the model performs much worse at identifying class $0$ compared to the other two classes. This phenomenon is called the class imbalance problem and is a result of the fact that some of our classes, like class $0$, are underrepresented in the training data. You can easily see this if you look at the [`label_img`](#1.-Multiclass-pixel-classification) image and think about how many of the total pixels belong to which class.\n",
    "\n",
    "## 1.D. Class imbalance problem\n",
    "[Back to index](#Index)\n",
    "\n",
    "First of all, let's check how well our three classes are actually represented in our dataset. Run the cell below to calculate the ratio each one of the classes compared to the total amount of pixels in our training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a6f7d5ec5d21160a179a58dd49b9c29",
     "grade": false,
     "grade_id": "cell-8fe973d5f5f41802",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of the number of pixels for each class:\n",
      "Class 0: 2.00%, Class 1: 38.29%, Class 2: 59.71%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, train_label_class_counts = np.unique(label_img.reshape((-1, 3)), axis=0, return_counts=True)\n",
    "# Compute class ratios\n",
    "class_ratio = train_label_class_counts/sum(train_label_class_counts)\n",
    "print(f'Ratio of the number of pixels for each class:\\nClass 0: {class_ratio[0]*100:.2f}%, Class 1: {class_ratio[1]*100:.2f}%, Class 2: {class_ratio[2]*100:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cc915f4fa9a07f5be933e0a3746ee7d",
     "grade": false,
     "grade_id": "cell-3c33bb42114373a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, class $0$ only represents $2\\%$ of all pixels in the training image, which makes it clear why our network had a hard time classifying it correctly.\n",
    "\n",
    "Unfortunately, this is a common issue in practice, especially in the medical imaging area, where abnormal data are rare compared to normal cases. The question remains, how can compensate for that? \n",
    "\n",
    "### 1.D.a. Add weights to the loss function (1 point)\n",
    "[Back to index](#Index)\n",
    "\n",
    "One of the easiest ways to address this issue is to add a weight $w_i$ to the loss function that corresponds to the representation of class $i$. Our new cross-entropy loss thus becomes: \n",
    "$$\\operatorname{H}(x, i) = w_i\\left(-\\log\\left(\\frac{e^{x_i}}{\\sum_{j=1}^C e^{x_j}}\\right)\\right)$$\n",
    "\n",
    "The final batch loss is then normalized for each minibatch of size $N$:\n",
    "\n",
    "$$\\operatorname{BatchLoss} =  \\frac{\\sum^{N}_{n=1}\\operatorname{H}(x^{(n)}, i^{(n)})}{\\sum^{N}_{n=1} w_{i^{(n)}}}.$$\n",
    "\n",
    "There are many different ways to calculate these weights, but here we will simply use <b>the inverse of the class ratios stored in the parameter `class_ratio`</b> that we calculated in the above cell. To add the weights to the cross-entropy loss function in PyTorch, you can simply use `loss_fn = torch.nn.CrossEntropyLoss(weight=weights)`, but this step is already implemented for you. \n",
    "\n",
    "All you need to do in the next cell, **for 1 point**, is to complete the function `calc_weights` that calculates the correct weights according to the parameter `class_counts`, an array of the size $(n, )$ (where $n$ is the number of classes) and returns the weights in the parameter `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ccdb55c961ac16fce459b326cb9768e",
     "grade": false,
     "grade_id": "cell-4ff8aaa82c7322e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_weights(class_counts):\n",
    "    # Calculate the correct weights\n",
    "    weights = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    weights = np.sum(class_counts)/class_counts\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Convert the weights to a tensor and re-create the loss function\n",
    "weights_tc = torch.FloatTensor(calc_weights(train_label_class_counts))\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ab0b3f253fb4e18994bfb5797e18054",
     "grade": false,
     "grade_id": "cell-0431f19b98456394",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's re-run the training with the new loss function and see if the classwise accuracy improved. Run the cell below to train your model again for 50 minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abf9cc9bbef6f584452c252941c8d738",
     "grade": true,
     "grade_id": "cell-93e5f12e10b8cbca",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1: train loss=  0.0541  valid_loss= 36.1793  valid_acc=58.58 %  best accuracy(@iter 0)=58.58\n",
      "Iteration  2: train loss=  0.3670  valid_loss= 41.2945  valid_acc= 1.98 %  best accuracy(@iter 0)=58.58\n",
      "Iteration  3: train loss=  0.2924  valid_loss= 28.6201  valid_acc= 1.98 %  best accuracy(@iter 2)= 1.98\n",
      "Iteration  4: train loss=  0.2119  valid_loss= 15.4676  valid_acc=59.53 %  best accuracy(@iter 3)=59.53\n",
      "Iteration  5: train loss=  0.0946  valid_loss=  8.8825  valid_acc=58.58 %  best accuracy(@iter 4)=58.58\n",
      "Iteration  6: train loss=  0.0819  valid_loss=  0.7181  valid_acc=84.13 %  best accuracy(@iter 5)=84.13\n",
      "Iteration  7: train loss=  0.0195  valid_loss=  5.2997  valid_acc=39.44 %  best accuracy(@iter 5)=84.13\n",
      "Iteration  8: train loss=  0.0545  valid_loss=  5.6879  valid_acc=39.44 %  best accuracy(@iter 5)=84.13\n",
      "Iteration  9: train loss=  0.0632  valid_loss=  4.1773  valid_acc=40.53 %  best accuracy(@iter 5)=84.13\n",
      "Iteration 10: train loss=  0.0401  valid_loss=  2.2657  valid_acc=41.18 %  best accuracy(@iter 5)=84.13\n",
      "Iteration 11: train loss=  0.0263  valid_loss=  0.6747  valid_acc=63.58 %  best accuracy(@iter10)=63.58\n",
      "Iteration 12: train loss=  0.0064  valid_loss=  1.2657  valid_acc=60.53 %  best accuracy(@iter10)=63.58\n",
      "Iteration 13: train loss=  0.0120  valid_loss=  1.6522  valid_acc=60.47 %  best accuracy(@iter10)=63.58\n",
      "Iteration 14: train loss=  0.0100  valid_loss=  1.7464  valid_acc=60.44 %  best accuracy(@iter10)=63.58\n",
      "Iteration 15: train loss=  0.0165  valid_loss=  1.6043  valid_acc=60.44 %  best accuracy(@iter10)=63.58\n",
      "Iteration 16: train loss=  0.0138  valid_loss=  1.3517  valid_acc=60.49 %  best accuracy(@iter10)=63.58\n",
      "Iteration 17: train loss=  0.0139  valid_loss=  1.1142  valid_acc=60.53 %  best accuracy(@iter10)=63.58\n",
      "Iteration 18: train loss=  0.0128  valid_loss=  0.8920  valid_acc=60.53 %  best accuracy(@iter10)=63.58\n",
      "Iteration 19: train loss=  0.0106  valid_loss=  0.7128  valid_acc=60.89 %  best accuracy(@iter10)=63.58\n",
      "Iteration 20: train loss=  0.0073  valid_loss=  0.6525  valid_acc=61.13 %  best accuracy(@iter19)=61.13\n",
      "Iteration 21: train loss=  0.0068  valid_loss=  0.6914  valid_acc=63.27 %  best accuracy(@iter19)=61.13\n",
      "Iteration 22: train loss=  0.0073  valid_loss=  0.7263  valid_acc=67.40 %  best accuracy(@iter19)=61.13\n",
      "Iteration 23: train loss=  0.0081  valid_loss=  0.7323  valid_acc=77.11 %  best accuracy(@iter19)=61.13\n",
      "Iteration 24: train loss=  0.0082  valid_loss=  0.7279  valid_acc=90.27 %  best accuracy(@iter19)=61.13\n",
      "Iteration 25: train loss=  0.0073  valid_loss=  0.7075  valid_acc=89.64 %  best accuracy(@iter19)=61.13\n",
      "Iteration 26: train loss=  0.0079  valid_loss=  0.6765  valid_acc=89.20 %  best accuracy(@iter19)=61.13\n",
      "Iteration 27: train loss=  0.0074  valid_loss=  0.6461  valid_acc=90.53 %  best accuracy(@iter26)=90.53\n",
      "Iteration 28: train loss=  0.0085  valid_loss=  0.6343  valid_acc=87.69 %  best accuracy(@iter27)=87.69\n",
      "Iteration 29: train loss=  0.0065  valid_loss=  0.6187  valid_acc=82.11 %  best accuracy(@iter28)=82.11\n",
      "Iteration 30: train loss=  0.0066  valid_loss=  0.5821  valid_acc=87.02 %  best accuracy(@iter29)=87.02\n",
      "Iteration 31: train loss=  0.0064  valid_loss=  0.5477  valid_acc=88.33 %  best accuracy(@iter30)=88.33\n",
      "Iteration 32: train loss=  0.0061  valid_loss=  0.5314  valid_acc=82.22 %  best accuracy(@iter31)=82.22\n",
      "Iteration 33: train loss=  0.0055  valid_loss=  0.5292  valid_acc=76.78 %  best accuracy(@iter32)=76.78\n",
      "Iteration 34: train loss=  0.0059  valid_loss=  0.5132  valid_acc=78.67 %  best accuracy(@iter33)=78.67\n",
      "Iteration 35: train loss=  0.0048  valid_loss=  0.4826  valid_acc=87.20 %  best accuracy(@iter34)=87.20\n",
      "Iteration 36: train loss=  0.0050  valid_loss=  0.4670  valid_acc=90.67 %  best accuracy(@iter35)=90.67\n",
      "Iteration 37: train loss=  0.0035  valid_loss=  0.4607  valid_acc=89.64 %  best accuracy(@iter36)=89.64\n",
      "Iteration 38: train loss=  0.0048  valid_loss=  0.4512  valid_acc=89.67 %  best accuracy(@iter37)=89.67\n",
      "Iteration 39: train loss=  0.0043  valid_loss=  0.4391  valid_acc=90.42 %  best accuracy(@iter38)=90.42\n",
      "Iteration 40: train loss=  0.0041  valid_loss=  0.4257  valid_acc=89.13 %  best accuracy(@iter39)=89.13\n",
      "Iteration 41: train loss=  0.0057  valid_loss=  0.4173  valid_acc=88.47 %  best accuracy(@iter40)=88.47\n",
      "Iteration 42: train loss=  0.0038  valid_loss=  0.4080  valid_acc=86.87 %  best accuracy(@iter41)=86.87\n",
      "Iteration 43: train loss=  0.0043  valid_loss=  0.3994  valid_acc=85.40 %  best accuracy(@iter42)=85.40\n",
      "Iteration 44: train loss=  0.0028  valid_loss=  0.3909  valid_acc=85.89 %  best accuracy(@iter43)=85.89\n",
      "Iteration 45: train loss=  0.0051  valid_loss=  0.3784  valid_acc=88.56 %  best accuracy(@iter44)=88.56\n",
      "Iteration 46: train loss=  0.0045  valid_loss=  0.3755  valid_acc=90.00 %  best accuracy(@iter45)=90.00\n",
      "Iteration 47: train loss=  0.0043  valid_loss=  0.3649  valid_acc=90.09 %  best accuracy(@iter46)=90.09\n",
      "Iteration 48: train loss=  0.0038  valid_loss=  0.3648  valid_acc=89.13 %  best accuracy(@iter47)=89.13\n",
      "Iteration 49: train loss=  0.0044  valid_loss=  0.3543  valid_acc=89.22 %  best accuracy(@iter48)=89.22\n",
      "Iteration 50: train loss=  0.0034  valid_loss=  0.3339  valid_acc=89.84 %  best accuracy(@iter49)=89.84\n",
      "\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the model for fair comparison, and link to new optimizer object  \n",
    "model = build_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "train_loss, valid_loss, valid_acc = train(model, train_loader, valid_loader, optimizer, loss_fn, max_iter=50, save_model_name='best_model_with_weights')\n",
    "print('\\nFinished training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7080533471a47c07e20d30a5741c05bb",
     "grade": false,
     "grade_id": "cell-45b80faec0a20de7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Did the overall accuracy improve? Run the cell below to visualize the training and validation loss history and the evolution of the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5369cad8b704ad138c3e2d17938310a1",
     "grade": false,
     "grade_id": "cell-ae6e6bc753a33b06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7085bce6564645c799b5a0ab5cb5af37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90.10 %\n"
     ]
    }
   ],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize = (8, 4))\n",
    "\n",
    "# Plot loss\n",
    "ax_loss = plt.gca()\n",
    "ax_loss.set_xlabel('Iterations')\n",
    "ax_loss.set_ylabel('Loss')\n",
    "plt.title('Training and Validation Loss and Accuracy History')\n",
    "p1 = ax_loss.plot(train_loss, 'y', label = 'Train Loss')\n",
    "p2 = ax_loss.plot(valid_loss, 'r', label = 'Val Loss')\n",
    "\n",
    "# Get twin axis and plot accuracy\n",
    "ax_acc = ax_loss.twinx()  \n",
    "ax_acc.set_ylabel('Accuracy') \n",
    "p3 = ax_acc.plot(valid_acc, 'k', label = 'Val Accuracy')\n",
    "legends = [l.get_label() for l in p1+p2+p3]\n",
    "plt.grid()\n",
    "ax_acc.legend(p1+p2+p3, legends)\n",
    "plt.show()\n",
    "\n",
    "# Load best model and set to evaluation mode\n",
    "model.load_state_dict(torch.load('best_model_with_weights.pt'))\n",
    "model.eval()\n",
    "# Calculate test accuracy\n",
    "y_hat = model(net_input_test)\n",
    "_, predicted = torch.max(y_hat.data, 1)\n",
    "correct = (predicted == net_target_test).sum().item()\n",
    "test_accuracy = correct / len(net_target_test) * 100\n",
    "print('Test accuracy: %.2f %%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "624d94a73acb16d73378d1cd77a17353",
     "grade": false,
     "grade_id": "cell-566db75dec1791e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now the total test accuracy might not have changed much, but if we calculate the classwise accuracy again, you should see a major improvement for class $0$. \n",
    "\n",
    "Run the cell below to do so and ensure that class $0$ is now classified much better than before. Moreover, we will the input image, the ground truth and the model output. The improvement should be evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e24248d3ed481dee2887d55852e0df95",
     "grade": false,
     "grade_id": "cell-b1301b8ae1553d96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (classwise)\n",
      "class 0: 95.44 % class 1: 74.80 % class 2: 96.40 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ccd6092bc14c3a906a231ffe83a5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991fb54c606d4daf81c4ea01ea1096a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate classwise accuracy\n",
    "cls0_acc_w =  100 * np.logical_and((predicted == 0).data.cpu(), (net_target_test ==0).data.cpu()).sum().item()/(net_target_test ==0).sum().item()\n",
    "cls1_acc_w = 100 * np.logical_and((predicted == 1).data.cpu(), (net_target_test ==1).data.cpu()).sum().item()/(net_target_test ==1).sum().item()\n",
    "cls2_acc_w = 100 * np.logical_and((predicted == 2).data.cpu(), (net_target_test ==2).data.cpu()).sum().item()/(net_target_test ==2).sum().item()\n",
    "print('Test accuracy (classwise)\\nclass 0: %.2f %% class 1: %.2f %% class 2: %.2f %%\\n' % (cls0_acc_w,cls1_acc_w,cls2_acc_w))\n",
    "# Create predicted image\n",
    "predicted_img = pixel_rgb_val[predicted.data.cpu().numpy()].reshape(input_img_test.shape)\n",
    "# Display\n",
    "plt.close('all')\n",
    "view = viewer([input_img_test, predicted_img, label_img_test], title=['Test image', 'Model output', 'Correct classification'], subplots=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1cc744305795ce714db00ff03db777e",
     "grade": false,
     "grade_id": "cell-b80d64c4f0cca561",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And finally, let's print both classwise accuracies together for an easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35c7d1d61d65d1070fa9c90cc22f5749",
     "grade": false,
     "grade_id": "cell-088817084e8437ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(without weights) class 0: 77.23 %, class 1: 59.39 %, class 2: 92.65 %\n",
      "(with weights)  class 0: 95.44 %, class 1: 74.80 %, class 2: 96.40 %\n"
     ]
    }
   ],
   "source": [
    "print(f'(without weights) class 0: {cls0_acc_wo:.2f} %, class 1: {cls1_acc_wo:.2f} %, class 2: {cls2_acc_wo:.2f} %')\n",
    "print(f'(with weights)  class 0: {cls0_acc_w:.2f} %, class 1: {cls1_acc_w:.2f} %, class 2: {cls2_acc_w:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3f29ef9144fef0aa2a6e037712f3c52",
     "grade": false,
     "grade_id": "cell-aadce1c4e8e2d419",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "<p><b>Congratulations on finishing the second part of the Neural Networks lab!</b></p>\n",
    "<p>\n",
    "Make sure to save your notebook (you might want to keep a copy on your personal computer) and upload it to <a href=\"https://moodle.epfl.ch/mod/assign/view.php?id=1157357\">Moodle</a>, in a zip file with other notebooks of this lab.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "* Keep the name of the notebook as: *2_NN_Application.ipynb*,\n",
    "* Name the zip file: *Neural_Networks_Lab.zip*.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<h4>Feedback</h4>\n",
    "    <p style=\"margin:4px;\">\n",
    "    This is the first edition of the image-processing laboratories using Jupyter Notebooks running on Noto. Do not leave before giving us your <a href=\"https://moodle.epfl.ch/mod/feedback/view.php?id=1157363\">feedback here!</a></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
