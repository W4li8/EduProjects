{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d43323667d62ffd06bb82242fdd86f4",
     "grade": false,
     "grade_id": "cell-9bf8f566ad985bf5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Image Processing Laboratory Notebooks</h2>\n",
    "<hr style=\"clear:both\">\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "This Juypter notebook is part of a series of computer laboratories which are designed\n",
    "to teach image-processing programming; they are running on the EPFL's Noto server. They are the practical complement of the theoretical lectures of the EPFL's Master course <b>Image Processing II</b> \n",
    "(<a href=\"https://moodle.epfl.ch/course/view.php?id=463\">MICRO-512</a>) taught by Dr. D. Sage, Dr. M. Liebling, Prof. M. Unser and Prof. D. Van de Ville.\n",
    "</p>\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "The project is funded by the Center for Digital Education and the School of Engineering. It is owned by the <a href=\"http://bigwww.epfl.ch/\">Biomedical Imaging Group</a>. \n",
    "The distribution or the reproduction of the notebook is strictly prohibited without the written consent of the authors.  &copy; EPFL 2021.\n",
    "</p>\n",
    "<p style=\"font-size:0.85em; margin:0px\"><b>Authors</b>: \n",
    "    <a href=\"mailto:pol.delaguilapla@epfl.ch\">Pol del Aguila Pla</a>, \n",
    "    <a href=\"mailto:kay.lachler@epfl.ch\">Kay Lächler</a>,\n",
    "    <a href=\"mailto:alejandro.nogueronaramburu@epfl.ch\">Alejandro Noguerón Arámburu</a>,\n",
    "    <a href=\"mailto:daniel.sage@epfl.ch\">Daniel Sage</a>, and\n",
    "    <a href=\"mailto:kamil.seghrouchni@epfl.ch\">Kamil Seghrouchni</a>.\n",
    "     \n",
    "</p>\n",
    "<hr style=\"clear:both\">\n",
    "<h1>Lab 4.2: Orientation</h1>\n",
    "<div style=\"background-color:#F0F0F0;padding:4px\">\n",
    "    <p style=\"margin:4px;\"><b>Released</b>: Thursday March 4, 2021</p>\n",
    "    <p style=\"margin:4px;\"><b>Submission</b>: <span style=\"color:red\">Friday March 12, 2021</span> (before 11:59PM) on <a href=\"https://moodle.epfl.ch/course/view.php?id=463\">Moodle</a></p>\n",
    "    <p style=\"margin:4px;\"><b>Grade weigth</b>: Lab 4 (18 points), 7.5 % of the overall grade</p>\n",
    "    <p style=\"margin:4px;\"><b>Remote help</b>: Monday 8 and Thursday 11 March, 2021 on Zoom (see Moodle for link)</p>    \n",
    "    <p style=\"margin:4px;\"><b>Related lectures</b>: Chapter 6</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name: \n",
    "### SCIPER: \n",
    "\n",
    "Double-click on this cell and fill your name and SCIPER number. Then, run the cell below to verify your identity in Noto and set the seed for random results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f33bf799b1c6d72b4286820b8994b161",
     "grade": true,
     "grade_id": "cell-8006fccc6af005fd",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIPER: 286557\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "# This line recovers your camipro number to mark the images with your ID\n",
    "uid = int(getpass.getuser().split('-')[2]) if len(getpass.getuser().split('-')) > 2 else ord(getpass.getuser()[0])\n",
    "print(f'SCIPER: {uid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dba74840cba7415a57263a5240f4e2e7",
     "grade": false,
     "grade_id": "cell-2a318fa77cf377ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <a name=\"imports_\"></a> Imports\n",
    "In the next cell we import Python libraries we will use throughout the lab, as well as the `IPLabViewer` class, created specifically for this course, which provides interactive image visualization based on the `ipywidgets` library:\n",
    "* [`matplotlib.pyplot`](https://matplotlib.org/3.2.2/api/_as_gen/matplotlib.pyplot.html), to display images\n",
    "* [`ipywidgets`](https://ipywidgets.readthedocs.io/en/latest/), to make the image display interactive\n",
    "* [`numpy`](https://numpy.org/doc/stable/reference/index.html), for mathematical operations on arrays\n",
    "* [`openCV` (cv2)](https://docs.opencv.org/2.4/index.html), for image-processing tasks\\n\",\n",
    "* [`scikit-image` (skimage)](https://scikit-image.org/docs/stable/api/api.html), also for image-processing tasks\\n\",\n",
    "\n",
    "We will then load the `IPLabViewer()` class (see documentation [here](https://github.com/Biomedical-Imaging-Group/IPLabImageViewer/wiki/Python-IPLabViewer()-Class), or run the Python commands`help(viewer)` after loading the class.)\n",
    "\n",
    "Finally, we load the images you will use in the exercise to test your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b337c5db94137968722a56599cb7c2b",
     "grade": false,
     "grade_id": "cell-267cd557273d688f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Configure plotting as dynamic\n",
    "%matplotlib widget\n",
    "\n",
    "# Import standard required packages for this exercise\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import cv2 as cv \n",
    "import skimage\n",
    "from skimage import feature\n",
    "# Import IPLabViewer() Class\n",
    "import sys  \n",
    "sys.path.insert(0, 'lib')\n",
    "from iplabs import IPLabViewer as viewer\n",
    "\n",
    "# Load images to be used in this exercise \n",
    "corner           = cv.imread('images/corner.tif',          cv.IMREAD_UNCHANGED)\n",
    "dendrochronology = cv.imread('images/dendrochronolgy.tif', cv.IMREAD_UNCHANGED)\n",
    "fingerprint      = cv.imread('images/fingerprint.tif',     cv.IMREAD_UNCHANGED)\n",
    "harris_corner    = cv.imread('images/harris-corner.tif',   cv.IMREAD_UNCHANGED)\n",
    "wave_ramp        = cv.imread('images/wave-ramp.tif',       cv.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05b0c542401c950dca5e31c21827a6d7",
     "grade": false,
     "grade_id": "cell-0d10bea2e8b42856",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Orientation laboratory (13 points)\n",
    "\n",
    "In this lab we will implement the computation of the structure tensor presented in Chapter 6.2, which can be used to perform directional image analysis.\n",
    "\n",
    "The block-diagram of the complete system is shown in the following flowchart, where $f(x,y)$ is the graylevel input image.\n",
    "\n",
    "<img src=\"images/block-diagram.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Successively, you will implement the functions\n",
    "* `structure_tensor` to generate the structure tensor matrix,\n",
    "* `orientation_features`, which implements the whole chain of calculations to generate the features needed for directional analysis, and\n",
    "* `colorize_features` to display the calculated features as a color image.\n",
    "\n",
    "\n",
    "Once you ensured that the functions are correct, you will use them in two applications that rely on directional image analysis, \n",
    "* a method to select specific orientations, and\n",
    "* a keypoint detector (Harris corner detector). \n",
    "\n",
    "Finally, we will analyze the `structure_tensor` function a bit more closely and see if we can improve the obtained results.\n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "\n",
    "**Note:** This part of the lab will be carried out completely in Python.\n",
    "</div>\n",
    "\n",
    "# Index\n",
    "1. [Structure tensor matrix](#1.-Structure-tensor-matrix-(2-points)) **(2 points)**\n",
    "2. [Orientation features](#2.-Orientation-features)\n",
    "    1. [Feature calculation](#2.A.-Feature-calculation-(4-points)) **(4 points)**\n",
    "    2. [Feature visualization](#2.B.-Feature-visualization-(2-points)) **(2 points)**\n",
    "3. [Application](#3.-Application)\n",
    "    1. [Selection of specific orientations](#3.A.-Selection-of-specific-orientations-(2-points)) **(2 points)**\n",
    "    2. [Harris corner detector](#3.B.-Harris-corner-detector-(2-points)) **(2 points)**\n",
    "    3. [*Advanced:* Isotropic filtering in the Fourier space](#3.C.-Advanced:-Isotropic-filtering-in-the-Fourier-space-(1-point)) **(1 point)**\n",
    "\n",
    "### Visualize images\n",
    "First of all, run the cell below to get familiar with the images we will be using. Remember you can use `Next` and `Prev` to cycle through the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb4889588d25bf88964dc3c8718d4b2f",
     "grade": false,
     "grade_id": "cell-752e79c9dd30f0a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faef0eaac63042a5904f7d891dfa7346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Declare image_list for ImageViewer\n",
    "img_list = [corner, dendrochronology, fingerprint, harris_corner, wave_ramp]\n",
    "imgs_viewer = viewer(img_list, widgets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2053d4b401dcf0eddbcc631738ab2c7c",
     "grade": false,
     "grade_id": "cell-5388b872e52f181d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Structure tensor matrix (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "To calculate the elements $J_{xx}$, $J_{xy}$ and $J_{yy}$ of the structure tensor, we only need two computational blocks: a Gaussian filter and a gradient filter. For the gradient, we will use the **OpenCV Sobel filter** [`cv.Sobel(src, ddepth, dx, dy, ksize)`](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d). Click on the link to see its documentation and choose the right input parameters to apply a **first-order** Sobel filter of **size $3\\times3$**. For the smoothing filter we will use the standard Gaussian filter provided by OpenCV [`cv.GaussianBlur(fxx, ksize, sigmaX)`](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) with default boundary conditions (`BORDER_DEFAULT` or `BORDER_REFLECT_101`). If you are unsure about its input parameters, go check the documentation.\n",
    "\n",
    "**For 2 points**, complete the function `structure_tensor` in the cell below. It may be useful to revisit the [figure](#Orientation-laboratory-(13-points)) at the start of this notebook before starting.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Hints:** \n",
    "* **`cv.Sobel`:** set `ddepth=cv.CV_64F` since the gradient can be a negative floating point number.\n",
    "* **`cv.GaussianBlur`:** set `ksize=(0,0)` to let the function choose the filter size automatically from `sigmaX`.\n",
    "* **Multiplication:** Note that, contrary to Matlab and other languages, the Python operator `*` applied to a NumPy Array performs *element-wise* multiplication.\n",
    "* To ensure that you define the right parameter in a function call always specify the name of the parameter. For example: `cv.Sobel(src=img, ddepth=cv.CV_64F, dx=1, dy=1, ksize=5)` is much easier to understand and debug than `cv.Sobel(img, cv.CV_64F, 1, 1, 5)`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc8d71f222545509d2b3a1031cf6bbfa",
     "grade": false,
     "grade_id": "cell-b61dd0d40e75e8f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculates the elements Jxx, Jxy and Jyy of the structure tensor matrix\n",
    "def structure_tensor(img, sigma):\n",
    "    Jxx = None\n",
    "    Jxy = None\n",
    "    Jyy = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    gradX = cv.Sobel(img, ddepth=cv.CV_64F, dx=1, dy=0, ksize=3)\n",
    "    gradY = cv.Sobel(img, ddepth=cv.CV_64F, dx=0, dy=1, ksize=3)\n",
    "    \n",
    "    gradXX = gradX**2\n",
    "    gradXY = gradX*gradY\n",
    "    gradYY = gradY**2\n",
    "    \n",
    "    Jxx = cv.GaussianBlur(gradXX, ksize=(0,0), sigmaX=sigma, borderType=cv.BORDER_DEFAULT)\n",
    "    Jxy = cv.GaussianBlur(gradXY, ksize=(0,0), sigmaX=sigma, borderType=cv.BORDER_DEFAULT)\n",
    "    Jyy = cv.GaussianBlur(gradYY, ksize=(0,0), sigmaX=sigma, borderType=cv.BORDER_DEFAULT)\n",
    "    \n",
    "    return Jxx, Jxy, Jyy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a30f4d5ebec78a97b72a9146189d293",
     "grade": false,
     "grade_id": "cell-c95fc6d29eace27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's perform a quick sanity check on a simple $11 \\times 11$ **impulse image** using `sigma=1`. \n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** You can modify the input image and the sigma value in the cell below to observe the different results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d4eec7891b43d2a4942f786ebd9d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e264a45c3d4a9ab95b1d0145119154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define impulse image\n",
    "size = 11\n",
    "test_img = np.zeros((size,size))\n",
    "test_img[size//2, size//2] = 1\n",
    "# Run function and display the result\n",
    "Jxx, Jxy, Jyy = structure_tensor(test_img, sigma = 1)\n",
    "plt.close('all')\n",
    "view = viewer([test_img, Jxx, Jyy, Jxy], subplots=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92ff06a9633b81f4a534ee5afda60d52",
     "grade": false,
     "grade_id": "cell-c03ed9b69a5e4658",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Specifically, we will first check that $J_{xx}$ and $J_{yy}$ are non-negative and identical to each other when rotated by $90^{\\circ}$, which should be the case for this impulse image. Then, we will also check that $J_{xy}$ contains both negative and non-negative numbers, and that all elements that are either in the fifth row or the fifth column of $J_{xy}$ are zero. \n",
    "\n",
    "Because the structure tensor is a crucial part of this lab, we will also perform more sophisticated sanity checks comparing your results to our pre-computed correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "593af240fcba0f38986e2e8e9eb62f50",
     "grade": true,
     "grade_id": "cell-3dd0c2f9710ea917",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! Your structure_tensor passed the sanity check.\n",
      "Remember that this is not a guarantee that everything is correct.\n"
     ]
    }
   ],
   "source": [
    "# Basic sanity checks\n",
    "# Re-run example in case you've played around with the previous cell\n",
    "test_img = np.zeros((11,11)); test_img[5, 5] = 1\n",
    "Jxx, Jxy, Jyy = structure_tensor(test_img, sigma = 1)\n",
    "\n",
    "# No negative values in Jxx and Jyy\n",
    "assert np.all(Jxx >= 0) and np.all(Jyy >= 0), '\\nJxx and Jyy should not contain any negative values.\\n'\n",
    "np.testing.assert_array_almost_equal(np.rot90(Jxx), Jyy, err_msg = \n",
    "                                     '\\nJxx should be the same as Jyy but rotated 90 degrees.\\n')\n",
    "\n",
    "# Jxy both positive and negative\n",
    "assert np.any(Jxy > 0) and np.any(Jxy < 0), '\\nJxy should contain both negative and positive values.\\n'\n",
    "\n",
    "# Fifth row/col zeros\n",
    "assert np.all(abs(Jxy[5, :]) < 1e-5) and np.all(abs(Jxy[:, 5]) < 1e-5), '\\nThe fifth row/column of Jxy should only have zeros.\\n'\n",
    "\n",
    "# Comparison to pre-computed correct results\n",
    "# Boundaries should be close to zero\n",
    "error_check = [False, False, False]\n",
    "mask = np.ones(test_img.shape, dtype=bool); mask[2:9, 2:9] = False\n",
    "if not np.all(np.abs(Jxx[mask]) < 0.01):\n",
    "    warnings.warn('Jxx is not yet correct, values outside the range x=[2,8], y=[2,8] should be close to 0.\\n')\n",
    "    error_check[0] = True\n",
    "if not np.all(np.abs(Jyy[mask]) < 0.01):\n",
    "    warnings.warn('Jyy is not yet correct, values outside the range x=[2,8], y=[2,8] should be close to 0.\\n')\n",
    "    error_check[1] = True\n",
    "if not np.all(np.abs(Jxy[mask]) < 0.01):\n",
    "    warnings.warn('Jxy is not yet correct, values outside the range x=[2,8], y=[2,8] should be close to 0.\\n')\n",
    "    error_check[2] = True\n",
    "Jxx_corr = np.array([[0.004, 0.018, 0.033, 0.035, 0.033, 0.018, 0.004],\n",
    "                     [0.025, 0.114, 0.209, 0.224, 0.209, 0.114, 0.025],\n",
    "                     [0.077, 0.35,  0.644, 0.688, 0.644, 0.35,  0.077],\n",
    "                     [0.113, 0.512, 0.942, 1.006, 0.942, 0.512, 0.113],\n",
    "                     [0.077, 0.35,  0.644, 0.688, 0.644, 0.35,  0.077],\n",
    "                     [0.025, 0.114, 0.209, 0.224, 0.209, 0.114, 0.025],\n",
    "                     [0.004, 0.018, 0.033, 0.035, 0.033, 0.018, 0.004]])\n",
    "Jyy_corr = Jxx_corr.T\n",
    "Jxy_corr = np.array([[ 0.003,  0.013,  0.019,  0.   , -0.019, -0.013, -0.003],\n",
    "                     [ 0.013,  0.056,  0.082,  0.   , -0.082, -0.056, -0.013],\n",
    "                     [ 0.019,  0.082,  0.119,  0.   , -0.119, -0.082, -0.019],\n",
    "                     [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
    "                     [-0.019, -0.082, -0.119,  0.   ,  0.119,  0.082,  0.019],\n",
    "                     [-0.013, -0.056, -0.082,  0.   ,  0.082,  0.056,  0.013],\n",
    "                     [-0.003, -0.013, -0.019,  0.   ,  0.019,  0.013,  0.003]])\n",
    "\n",
    "# Exact values in the center\n",
    "try:\n",
    "    np.testing.assert_array_almost_equal(Jxx[2:9, 2:9], Jxx_corr, decimal=3)\n",
    "except:\n",
    "    warnings.warn('The non-zero values of Jxx inside the range x=[2,8], y=[2,8] are not yet correct. Compare your output from above with the correct version below.\\n')\n",
    "    error_check[0] = True\n",
    "try:\n",
    "    np.testing.assert_array_almost_equal(Jyy[2:9, 2:9], Jyy_corr, decimal=3)\n",
    "except:\n",
    "    warnings.warn('The non-zero values of Jyy inside the range x=[2,8], y=[2,8] are not yet correct. Compare your output from above with the correct version below.\\n')\n",
    "    error_check[1] = True\n",
    "try:\n",
    "    np.testing.assert_array_almost_equal(Jxy[2:9, 2:9], Jxy_corr, decimal=3)\n",
    "except:\n",
    "    warnings.warn('The non-zero values of Jxy inside the range x=[2,8], y=[2,8] are not yet correct. Compare your output from above with the correct version below.\\n')\n",
    "    error_check[2] = True\n",
    "\n",
    "# In the presence of errors, show differences to debug your code\n",
    "if np.any(error_check):\n",
    "    Jxx_vis = np.zeros(test_img.shape); Jyy_vis = np.zeros(test_img.shape); Jxy_vis = np.zeros(test_img.shape)\n",
    "    Jxx_vis[mask == False] = Jxx_corr.flatten(); Jyy_vis[mask == False] = Jyy_corr.flatten(); Jxy_vis[mask == False] = Jxy_corr.flatten()\n",
    "    corr_imgs = [Jxx_vis, Jyy_vis, Jxy_vis]; imgs = [Jxx, Jyy, Jxy]; names = ['Jxx', 'Jyy', 'Jxy']\n",
    "    img_list = []; title_list = []; err_count = 0\n",
    "    for i, c in enumerate(error_check):\n",
    "        if c:\n",
    "            img_list.append(imgs[i]); img_list.append(corr_imgs[i])\n",
    "            title_list.append(names[i]); title_list.append(names[i] + ' correct')\n",
    "            err_count += 1\n",
    "    plt.close('all'); view = viewer(img_list, title=title_list, subplots=(err_count,2))\n",
    "else:\n",
    "    print('Congratulations! Your structure_tensor passed the sanity check.') \n",
    "    print('Remember that this is not a guarantee that everything is correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aacd731ac391bbf90426ed94282fdbcc",
     "grade": false,
     "grade_id": "cell-03e0e8556b98e0e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now you can also apply `structure_tensor` to the various images we have imported before (corner, dendrochronology, fingerprint, harris_corner, or wave_ramp) and see what the elements of the structure tensor matrix look like. Run the cell below and change the variable `image` to different images. You can also change the `sigma` value and observe the effect this has on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50592dd8d94439bb2518cea27a8566c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f12b7f2665a4beb9e003c9ea6caf25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you can change the image to any of the ones we imported: corner, dendrochronology, fingerprint, harris_corner, or wave_ramp\n",
    "image = wave_ramp\n",
    "\n",
    "Jxx, Jxy, Jyy = structure_tensor(image, sigma = 2)\n",
    "plt.close('all')\n",
    "view = viewer([image, Jxx, Jxy, Jyy], subplots=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18bab7cf52f9fe52052ea83b999cd41e",
     "grade": false,
     "grade_id": "cell-61a2e3d706807da8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2. Orientation features\n",
    "[Back to index](#Index)\n",
    "\n",
    "Now that we have the structure tensor, we can calculate interesting features from it that help us understand and visualize the orientation of an image. An easy way to think about the structure tensor is that, for each pixel location `[m,n]`, we can calculate a matrix $\\mathbf{J}$ made out of the values of $J_{xx}$, $J_{yy}$, and $J_{xy}$ at that same pixel, i.e., \n",
    "\n",
    "$$\n",
    "    \\mathbf{J}[m,n] = \\left[ \\begin{array}{cc} J_{xx}[m,n] & J_{xy}[m,n] \\\\ J_{xy}[m,n] & J_{yy}[m,n]\\end{array} \\right]\\,.\n",
    "$$\n",
    "\n",
    "## 2.A. Feature calculation (4 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "In the table below, you can see the four features that we are going to compute from this matrix $\\mathbf{J}$, where we drop the pixel indeces `[m,n]` for simplicity. There, $\\det(\\mathbf{J})$ stands for the determinant of the matrix, and $\\operatorname{tr}(\\mathbf{J})$ for the trace.\n",
    "\n",
    "| Feature | Relation to the structure tensor matrix $\\mathbf{J}$ |\n",
    "| :-: | :-: |\n",
    "| Orientation | $$\\Theta = \\frac{1}{2}\\arctan\\left(\\frac{2J_{xy}}{J_{yy}-J_{xx}}\\right)$$ |\n",
    "| Gradient Energy | $$E = J_{yy}+J_{xx}$$ |\n",
    "| Coherence | $$C = \\frac{\\sqrt{(J_{yy}-J_{xx})^2+4J_{xy}^2}}{J_{yy}+J_{xx}}$$ |\n",
    "| Harris Index | $$H = \\det(\\mathbf{J}) - \\kappa \\operatorname{tr}(\\mathbf{J})^2\\mbox{, with }\\kappa = 0.05$$ |\n",
    "\n",
    "In the cell below, write the function `orientation_features()` which implements the whole chain of processing of the algorithm described in the [flowchart](#Orientation-laboratory-(13-points)) at the start of this lab. Use the `structure_tensor` function you prepared in the previous section to get the structure tensor and use the relations shown in the table above to calculate the specified features (**1 point each**).\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note:** For $\\arctan\\left(\\frac{x1}{x2}\\right)$ in $[-\\pi, \\pi]$ use `np.arctan2(x1, x2)`.\n",
    "</div>\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Beware:** At each pixel where $J_{xx}[m,n]+J_{yy}[m,n] < 0.01$, set the **coherence to 0** to avoid a division by zero.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc8f7990035205d441a8afb0cac7ec3b",
     "grade": false,
     "grade_id": "cell-035389b1a6a3f768",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec41525ac3d40bb9ad88be0786a85f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that calculates the orientation features of an image for a given sigma\n",
    "def orientation_features(img, sigma):\n",
    "    orientation = np.zeros(img.shape)\n",
    "    energy      = np.zeros(img.shape)\n",
    "    coherence   = np.zeros(img.shape)\n",
    "    harris      = np.zeros(img.shape)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    Jxx, Jxy, Jyy = structure_tensor(img, sigma)\n",
    "    detJ = Jxx*Jyy - Jxy**2  #\n",
    "    trJ  = Jxx+Jyy  # \n",
    "\n",
    "    orientation = 0.5*np.arctan2(2*Jxy, Jyy-Jxx)\n",
    "    energy = trJ\n",
    "    # coherence = np.where(trJ < 0.01, 0, np.sqrt((Jyy-Jxx)**2 + 4*Jxy**2) / trJ)  # division by 0 warning although perfectly functional\n",
    "    coherence[trJ >= 0.01] = np.sqrt((Jyy-Jxx)**2 + 4*Jxy**2)[trJ >= 0.01] / trJ[trJ >= 0.01]  # avoid division by 0 warning, using zero-init\n",
    "    harris = detJ - 0.05*trJ**2 \n",
    "\n",
    "    return orientation, energy, coherence, harris\n",
    "\n",
    "# Change the input image and sigma, to see the result on different images: corner, dendrochronology, fingerprint, harris_corner, or wave_ramp\n",
    "input_img = wave_ramp\n",
    "sigma = 2\n",
    "image_list = [input_img] + list(orientation_features(input_img, sigma=sigma))\n",
    "# Display the output features\n",
    "title_list = ['Input image', 'Orientation', 'Energy', 'coherence', 'Harris Index']\n",
    "plt.close('all')\n",
    "view = viewer(image_list, title = title_list, colorbar=True, widgets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdb46c03e9ee03cc15fe3cde7e378b97",
     "grade": false,
     "grade_id": "cell-a73ac8dca01efbe7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a sanity check, you can run the next four cells to check that each of the output features is in the correct range when applying the function to the `wave_ramp` image using `sigma = 2`. This does not mean that everything is correct but it can help to detect basic calculation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b13abe6c2a227a56e900e943c2cb4f2",
     "grade": true,
     "grade_id": "cell-3ca60c23375ccae0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The orientation is in the correct range.\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "features = orientation_features(wave_ramp, sigma=2)\n",
    "# Orientation\n",
    "assert abs(features[0].min() + np.pi/2) < 0.01 and abs(features[0].max() - np.pi/2) < 0.01, \\\n",
    "       f'The orientation should be in [-pi/2, pi/2], not in [{features[0].min():.3f}, {features[0].max():.3f}]'\n",
    "print('The orientation is in the correct range.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15118b6f38ab83e726300991eedac61b",
     "grade": true,
     "grade_id": "cell-5668c02734169a3d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The energy is in the correct range for this image.\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "features = orientation_features(wave_ramp, sigma=2)\n",
    "# Energy\n",
    "assert abs(features[1].min() - 17) < 1 and abs(features[1].max() - 29) < 1, \\\n",
    "       f'For this image, the energy should be in the range [~17, ~29], but it is in [{features[1].min():.3f}, {features[1].max():.3f}]'\n",
    "print('The energy is in the correct range for this image.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90e4077f56ac9d9679ecf7e6b1ba003a",
     "grade": true,
     "grade_id": "cell-873a5671c9e98f31",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coherence is in the correct range.\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "features = orientation_features(wave_ramp, sigma=2)\n",
    "# coherence\n",
    "assert abs(features[2].min()) < 0.01 and abs(1 - features[2].max()) < 0.01, \\\n",
    "       f'The coherence should be in the range [0, 1], not in [{features[2].min():.3f}, {features[2].max():.3f}]'\n",
    "print('The coherence is in the correct range.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1517c9dfd97b4d55a64f9457d2c18d3",
     "grade": true,
     "grade_id": "cell-09db35e08a4e2c57",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Harris index is in the correct range.\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "features = orientation_features(wave_ramp, sigma=2)\n",
    "# Harris index\n",
    "assert abs(features[3].min() + 41.5) < 1 and abs(features[3].max() - 111.3) < 1, \\\n",
    "       f'For this image, the Harris index should be in the range [~-41.5, ~111.3], not in [{features[3].min():.3f}, {features[3].max():.3f}]'\n",
    "print('The Harris index is in the correct range.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11255806ed3a1fa822880050cb54e456",
     "grade": false,
     "grade_id": "cell-98e9a94fd283428b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.B. Feature visualization (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Until now, we used grayscale (2D) images to visualize the orientation features. This only allows for single features to be displayed at once. It is often convenient to be able to visualize several features at once and integrate them into the original image to make the visual analysis more intuitive. One way to achieve this is to use the **hue, saturation, value (HSV)** color representation, which is sometimes also called HSB (for brightness), and is depicted in the figure below. Using this method, we can assign the orientation (a $2\\pi$ periodic value) to the hue (which also lives in a circular scale), the coherence (a value between $0$ and $1$) to the saturation, and the brightness of the original image to the value, which will allow us to see the objects in the image. This will result on much clearer representations of the orientation features.\n",
    "\n",
    "<img src=\"images/hsv_color_representation.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "In the cell below, **for 2 points**, implement the function `colorize_features` that takes as input parameters the *orientation*, the *coherence*, the *input image*, and a *mode* (of two possible modes, see table below). The function should\n",
    "1. create an HSV image `hsv_image` from the orientation features, and\n",
    "2. convert this HSV image to an RGB image using `rgb_img = cv.cvtColor(hsv_img, cv.COLOR_HSV2RGB)` in order to display it with the `viewer`.\n",
    " \n",
    "Below, both `hsv_img` and `rgb_img` are 3D arrays with dimensions `(height, width, channels)` where the $i$-th channel can be accessed as `img[:, :, i]` and it contains the information relative to the letter in position $i$ of the name of the respective color representation (HSV or RGB: $i=0$, hue or red channel, $i=1$, saturation or green channel, $i=2$, value or blue channel). \n",
    "\n",
    "The function will have two modes. Mode $0$ simply maps the orientation to the hue channel, while mode $1$ overlays the orientation and coherence features on top of the input image. See the table below for the specification of the two modes.  \n",
    "\n",
    "| Mode | H channel, range `[0, 180]` | S channel, range `[0, 255]` | V channel, range `[0, 255]` |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0: Orientation only | orientation | constant image of value `255` | constant image of value `255` |\n",
    "| 1: Features on image | orientation | coherence | input image |\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Beware:** Think carefully about how each channel should (or should not) be shifted and/or normalized. Any two pixels with the same brightness, orientation and coherence features in different images should look the same! Note that the orientation as you computed it above is in the range $[-\\pi/2,\\pi/2]$, and the coherence is in the range $[0,1]$. Assume the input image is in the range $[0,255]$.\n",
    "</div>\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Beware:** In order for the `cvtColor` function to work, it's important that the images are of type `uint8`. Please simply modify the predefined variable `hsv_img` to create the HSV image, and store the RGB image in the predefined variable `rgb_img`.\n",
    "</div>\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** If you are curious about how the conversion from HSV to RGB works, you can check out the formula [here](https://en.wikipedia.org/wiki/HSL_and_HSV#HSV_to_RGB). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "354100a5281a06ae188568023693e4b3",
     "grade": false,
     "grade_id": "cell-95da8e71f5180d8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that returns a colorized rgb image depending on the orientation features\n",
    "def colorize_features(orientation, coherence, img, mode):\n",
    "    # Fill hsv_img[:,:,0] to set the hue, hsv_img[:,:,1] to set the saturation, and hsv_img[:,:,2] to set the value\n",
    "    hsv_img = np.zeros((img.shape[0], img.shape[1], 3), dtype = np.uint8)\n",
    "    rgb_img = np.zeros((img.shape[0], img.shape[1], 3), dtype = np.uint8)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    hsv_img[:,:,0] = orientation*180/np.pi + 90\n",
    "    if mode == 1: \n",
    "        hsv_img[:,:,1] = coherence*255\n",
    "        hsv_img[:,:,2] = img\n",
    "    else: \n",
    "        hsv_img[:,:,1] += 255\n",
    "        hsv_img[:,:,2] += 255\n",
    "        \n",
    "    rgb_img = cv.cvtColor(hsv_img, cv.COLOR_HSV2RGB)\n",
    "    return rgb_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b6b8c4b45d741a23e8755135dab6c51",
     "grade": false,
     "grade_id": "cell-674dd871665d5425",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now run the next two cells for a quick test on your function. As usual, remember that these tests are not definitive and that they do not guarantee the full points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58e5ceeca49695824367e5c4908b487f",
     "grade": true,
     "grade_id": "cell-1f013b9d212f4ead",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done, your colorization function passed the sanity check for mode 0.\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for mode 0. First we define a few arrays for which we know how the output will look like.\n",
    "orientation = np.array([[-np.pi/2, -np.pi/4, 0, np.pi/4, np.pi/2]])\n",
    "coherence   = np.array([[0, 65./255, 130./255, 195./255, 255./255]])\n",
    "img = np.array([[255, 195, 130, 65, 0]])\n",
    "colorized_img = colorize_features(orientation, coherence, img, mode=0)\n",
    "check_img = np.array([[[255, 0, 0], [127, 255, 0], [0, 255, 255],[128,   0, 255], [255,   0,   0]]], dtype=np.uint8)\n",
    "try:\n",
    "    np.testing.assert_array_almost_equal(colorized_img, check_img)\n",
    "except AssertionError:\n",
    "    warnings.warn('\\n\\nSorry, your colorization function is not yet correct for mode 0.\\n')\n",
    "    plt.close('all')\n",
    "    view = viewer([check_img, colorized_img], title=['Expected output', 'Your output'], subplots=(1,2))\n",
    "else:\n",
    "    print('Well done, your colorization function passed the sanity check for mode 0.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "945c81787689003d8fc5337b14ec10b9",
     "grade": true,
     "grade_id": "cell-0c20e2a70ef7e9e3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done, your colorization function passed the sanity check for mode 1.\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for mode 1\n",
    "orientation = np.array([[-np.pi/2, -np.pi/4, 0, np.pi/4, np.pi/2]])\n",
    "coherence   = np.array([[0, 65./255, 130./255, 195./255, 255./255]])\n",
    "img = np.array([[255, 195, 130, 65, 0]])\n",
    "colorized_img = colorize_features(orientation, coherence, img, mode=1)\n",
    "check_img = np.array([[[255, 255, 255], [170, 195, 145], [64, 130, 130],[40,  15,  65], [0, 0, 0]]], dtype=np.uint8)\n",
    "try:\n",
    "    np.testing.assert_array_almost_equal(colorized_img, check_img)\n",
    "except AssertionError:\n",
    "    warnings.warn('\\n\\nSorry, your colorization function is not yet correct for mode 1.\\n')\n",
    "    plt.close('all')\n",
    "    view = viewer([check_img, colorized_img], title=['Expected output', 'Your output'], subplots=(1,2))\n",
    "else:\n",
    "    print('Well done, your colorization function passed the sanity check for mode 1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d5634b4a499a24071e684a097a040db",
     "grade": false,
     "grade_id": "cell-fddbf52cab001282",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to visualize the effect of the `colorize_features()` function on different images. \n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** Click on `Extra Widgets` to change the mode and sigma values and apply the colorization by clicking on `Apply Colorization`. Cycle through the different images by clicking on `Next` and `Prev`.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a847bdf3ac4eb4bebcba6d03ece06c83",
     "grade": false,
     "grade_id": "cell-4babc8c2b2129b2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f6053efacd41f28bc9fca9fa54ab7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define control widgets for \"Extra Widgets\"\n",
    "mode_dropdown = widgets.Dropdown(options=['0: Orientation only', '1: Features on image'],value='1: Features on image',description='Mode:',disabled=False)\n",
    "mode_dictionary = {'0: Orientation only':0, '1: Features on image':1}\n",
    "sigma_slider = widgets.IntSlider(value = 3, min = 1, max = 15, step = 1, description = r'$\\sigma$')\n",
    "button = widgets.Button(description = 'Apply Colorization')\n",
    "\n",
    "def colorization_callback(img):\n",
    "    mode = mode_dictionary[mode_dropdown.value]\n",
    "    sigma = sigma_slider.value\n",
    "    # Get the features using the function from part 2.1\n",
    "    features = orientation_features(img, sigma=sigma)\n",
    "    # Create the colorized image\n",
    "    output = colorize_features(features[0], features[2], img, mode=mode)\n",
    "    return output\n",
    "\n",
    "plt.close('all')\n",
    "image_list = [255*(wave_ramp-wave_ramp.min())/(wave_ramp.max()-wave_ramp.min()), dendrochronology, fingerprint]\n",
    "title = [\"wave_ramp\", \"dendrochronology\", \"fingerprint\"]\n",
    "new_widgets = [mode_dropdown, sigma_slider, button]\n",
    "view = viewer(image_list, new_widgets=new_widgets, callbacks=[colorization_callback], widgets=True, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e1ec8a70f65db378719cc6015f3147c",
     "grade": false,
     "grade_id": "cell-f282f7edfcdf4135",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 3. Application\n",
    "[Back to index](#Index)\n",
    "\n",
    "In this section you will implement some applications that rely on the functions you implemented in the previous sections, in order to show you what they could be used for in real-life scenarios.\n",
    "\n",
    "## 3.A. Selection of specific orientations (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "We propose to develop a function that only selects areas of the image with a specific orientation. In particular, the algorithm has to preserve pixels which have the following structure-tensor features,\n",
    " - $E > T E_{max}$, where $E_{max}$ is the maximum energy in the image and $T\\in[0,1]$ is the relative threshold,\n",
    " - $C > 0.5$,\n",
    " - $\\theta_{min} \\leq \\theta(x, y) \\leq \\theta_{max}$, with $\\theta$ in radians.\n",
    "\n",
    "In the next cell, **for 2 points**, implement the function `select_direction(img, sigma, T, theta_min, theta_max)` that takes as input parameters:\n",
    "* `img`: The input image\n",
    "* `sigma`: $\\sigma$ to be used in `orientation_features`\n",
    "* `T`: The relative energy threshold\n",
    "* `theta_min`: The minimum angle $\\theta_{min}$\n",
    "* `theta_max`: The maximum angle $\\theta_{max}$\n",
    "\n",
    "and that returns:\n",
    "* `output`: Output image keeping the pixels with the given features, with all other pixels set to the minimum value of the image (which is not always 0). \n",
    "\n",
    "Use the function `orientation_features` you implemented in [Part 2.A.](#2.A.-Feature-calculation-(4-points)) to get the features needed.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Note:** Since the angle $\\theta$ is periodic, it is not always clear which angle is larger. You should account for the fact that $\\theta_{\\mathrm{min}}$ can be larger than $\\theta_{\\mathrm{max}}$: If $\\theta_{\\mathrm{min}} \\leq \\theta_{\\mathrm{max}}$ then return the values inside the range [$\\theta_{\\mathrm{min}}$, $\\theta_{\\mathrm{max}}$], otherwise return the values that are outside this range, i.e., $[-\\pi/2,\\pi/2]\\setminus (\\theta_{\\mathrm{max}}, \\theta_{\\mathrm{min}})$ (see [here](https://en.wikipedia.org/wiki/Complement_(set_theory)#Relative_complement)). For example: if $\\theta_{\\mathrm{min}} = \\frac{\\pi}{3}$ and $\\theta_{\\mathrm{max}} = -\\frac{\\pi}{3}$ the function should keep all orientation in the ranges $[\\frac{\\pi}{3}, \\frac{\\pi}{2}]$ and $[-\\frac{\\pi}{2}, -\\frac{\\pi}{3}]$ but discard all orientations in the range $(-\\frac{\\pi}{3}, \\frac{\\pi}{3})$. Drawing this example will certainly help you to understand this better.\n",
    "</div>\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Hint:** You can use [`np.logical_and(condition_1, condition_2)`](https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html) to get the boolean array for which both `condition_1` and `condition_2` are true.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "200de9705d47bab13483b91c06da27e1",
     "grade": false,
     "grade_id": "cell-7ebcdf2e46ec9843",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that extracts a range of orientations given by theta_min and theta_max\n",
    "def select_direction(img, sigma, T, theta_min, theta_max):\n",
    "    # Check input params\n",
    "    assert 0 < T < 1, 'The threshold should be between 0 and 1'\n",
    "    assert -np.pi/2 <= theta_min <= np.pi/2, 'theta_min should be in [-pi/2, pi/2]'\n",
    "    assert -np.pi/2 <= theta_max <= np.pi/2, 'theta_max should be in [-pi/2, pi/2]'\n",
    "    \n",
    "    output = img.copy()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    orientation, energy, coherence, harris = orientation_features(img, sigma)\n",
    "    \n",
    "    if theta_min <= theta_max:\n",
    "        output = np.where(np.logical_and(np.logical_and(energy > T*np.max(energy), coherence > 0.5),\\\n",
    "                                         np.logical_and(theta_min <= orientation, orientation <= theta_max)), img, np.min(img))\n",
    "    else:\n",
    "        output = np.where(np.logical_and(np.logical_and(energy > T*np.max(energy), coherence > 0.5),\\\n",
    "                                         np.logical_or(theta_min <= orientation, orientation <= theta_max)), img, np.min(img))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf3d3673013afb2cedf5f60fa7b9fe81",
     "grade": false,
     "grade_id": "cell-d4adac96d837cb9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell will evaluate your function on a test image that consists of 4 lines at the angles $0$, $\\frac{\\pi}{4}$, $-\\frac{\\pi}{4}$ and $\\frac{\\pi}{2}$. The function will be called on this test image with the ranges $[-\\frac{\\pi}{6}, \\frac{\\pi}{6}]$, $[\\frac{\\pi}{6}, \\frac{\\pi}{3}]$, $[-\\frac{\\pi}{3}, -\\frac{\\pi}{6}]$, and $[\\frac{\\pi}{3}, -\\frac{\\pi}{3}]$, which should each exctract only one of the lines. Run the cell below to apply this sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76de1828e7fdce2af875f48094232f65",
     "grade": true,
     "grade_id": "cell-80bb1e523f292dcb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9537606480d542529d3923a656c9f227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded42186834b4af89df0dee5c45d842d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done, your function passed the sanity check!\n"
     ]
    }
   ],
   "source": [
    "# Create test image consisting of 4 lines at 0, pi/2, pi/4 and -pi/4\n",
    "n = 51; r = n//2\n",
    "line0 = np.zeros((n,n)); line0[r, :r-6] = 1; line0[r, r+7:] = 1\n",
    "line90 = np.zeros((n,n)); line90[:r-6, r] = 1; line90[r+7:, r] = 1\n",
    "line45 = np.zeros((n,n)); line45[range(n-1, r+6, -1), range(r-6)] = 1\n",
    "line45[range(r-7, -1, -1), range(r+7, n)] = 1\n",
    "lineM45 = np.zeros((n,n)); lineM45[range(r-6), range(r-6)] = 1\n",
    "lineM45[range(r+7, n), range(r+7, n)] = 1\n",
    "test_img = line0 + line90 + line45 + lineM45\n",
    "plt.close('all')\n",
    "view = viewer(test_img)\n",
    "\n",
    "# Test the 4 lines\n",
    "lines = [line0, line45, lineM45, line90]\n",
    "ranges = [[-np.pi/6, np.pi/6], [np.pi/6, np.pi/3], [-np.pi/3, -np.pi/6], [np.pi/3, -np.pi/3]]\n",
    "names = ['horizontal', 'diagonal ascending', 'diagonal descending', 'vertical']\n",
    "check = True\n",
    "for i, ran in enumerate(ranges):\n",
    "    test_dir = select_direction(test_img, T=0.1, sigma=2, theta_min=ran[0], theta_max=ran[1])\n",
    "    try:\n",
    "        np.testing.assert_array_equal(test_dir, lines[i])\n",
    "    except AssertionError:\n",
    "        check = False\n",
    "        warnings.warn(f'\\n\\nOnly the {names[i]} line should be visible at \\\n",
    "                      theta_min = {ran[0]:.4f}, theta_max = {ran[1]:.4f}!\\n')\n",
    "        image_list = [test_img, lines[i], test_dir, np.abs(lines[i]-test_dir)]\n",
    "        title_list = ['Test Image', f'Expected output for min = {ran[0]:.4f}, max = {ran[1]:.4f}', \n",
    "                      f'Your output', 'Difference']\n",
    "        view = viewer(image_list, title = title_list, subplots=(2,2))\n",
    "if check:\n",
    "    print('Well done, your function passed the sanity check!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea4df2c05aba3fda0b6424ce75432000",
     "grade": false,
     "grade_id": "cell-4b31587ba3f17bae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell to apply your function to the images we have been working with and play around with the different parameters by clicking on the button *Extra Widgets*. You can cycle through the different images by clicking on *Next* and *Prev*. Note that for some images the threshold  $T$ needs to be set very low in order to extract any orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b46e8a6cafe915a38aa10b5f02d3c23",
     "grade": false,
     "grade_id": "cell-8c2eebd3aac72aa7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239f1966082f4888a228188b597d967f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Sliders and button\n",
    "min_slider = widgets.FloatSlider(value = -np.pi/2, min = -np.pi/2, max = np.pi/2, step=0.01, description = r'$\\theta_\\mathrm{min}$')\n",
    "max_slider = widgets.FloatSlider(value = np.pi/2, min = -np.pi/2, max = np.pi/2, step=0.01, description = r'$\\theta_\\mathrm{max}$')\n",
    "T_slider = widgets.FloatSlider(value = 0.5, min = 0.05, max = 0.95, step=0.05, description = r'$T$')\n",
    "sigma_slider = widgets.IntSlider(value = 3, min = 1, max = 15, step = 1, description = r'$\\sigma$')\n",
    "button = widgets.Button(description = 'Extract Orientation')\n",
    "\n",
    "# Define Callback function\n",
    "def orientation_callback(img):\n",
    "    theta_min = min_slider.value\n",
    "    theta_max = max_slider.value\n",
    "    T = T_slider.value\n",
    "    sigma = sigma_slider.value\n",
    "    # Create the colorized image\n",
    "    output = select_direction(img, sigma, T, theta_min, theta_max)\n",
    "    return output\n",
    "\n",
    "# Declare viewer parameters and start the visualization\n",
    "plt.close('all')\n",
    "image_list = [wave_ramp, dendrochronology, fingerprint]\n",
    "new_widgets = [sigma_slider, T_slider, min_slider, max_slider, button]\n",
    "view = viewer(image_list, new_widgets = new_widgets, callbacks = [orientation_callback], widgets = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "104384ce2a330b198a5a83083f837f8b",
     "grade": false,
     "grade_id": "cell-906043185ffbce3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.B. Harris corner detector (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "As you may remember from the lecture, the Harris index can be interpreted as the probability of having a corner at the corresponding location. This means we can implement a very basic corner detector by extracting the local maxima of the Harris index image and using them as the locations for our corners.\n",
    "\n",
    "**For 1 point**, complete the function `detect_corners()` in the cell below. This function takes as parameters\n",
    " * `img`: An image where to detect corners, in the form of a 2D NumPy array,  \n",
    " * `L`: the size (in integer number of pixels) of the square observation region around each pixel (that defines what a _local maxima_ is), and\n",
    " * `T`: a relative threshold in the range $[0,1]$, where only local maxima that are above $T$ times the image maximum are kept. \n",
    " \n",
    "The function returns:\n",
    " * `output`: A list of coordinates where a local maximum has been identified, as specified by the `peak_local_max` function (see the hint below).\n",
    " \n",
    "<div class = 'alert alert-info'>\n",
    "\n",
    "**Hints:**\n",
    " * To get the Harris index image, use the function `orientation_features` that you implemented in [Part 2.A](#2.-Orientation-features) **with `sigma=1`**.\n",
    " * To extract the local maxima, use the function [`peak_local_max`](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.peak_local_max) provided by `skimage.feature`. It should be called as `feature.peak_local_max(input_img, min_distance, threshold_rel)`. \n",
    " * The parameter `threshold_rel` (in the function `peak_local_max`) is given by the input parameter `T`.\n",
    " * The parameter `min_distance` (in the function `peak_local_max`) is the minimum distance separating two local maxima in an $L\\times L$ region. It is given by the relation $L = 2\\;*$ `min_distance` $+ 1$. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c30f83a01d251062e230a2c700332e7",
     "grade": false,
     "grade_id": "cell-dec1d2e15105646e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that detects corners using the Harris index method\n",
    "def detect_corners(img, L, T):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    orientation, energy, coherence, harris = orientation_features(img, sigma=1)\n",
    "\n",
    "    output = feature.peak_local_max(harris, min_distance=(L-1)//2, threshold_rel=T)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b2b75d1a215409a4bea8c84ec77aedd",
     "grade": false,
     "grade_id": "cell-7bb80fe876c4c8b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below define the function `show_red_crosses`, which produces a nice RGB image to visualize the results of the Harris corner detector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5340e781b89f76755afc535a16de4194",
     "grade": false,
     "grade_id": "cell-56b58590168cfc26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function to prepare a nice visualization for Harris corners' detections\n",
    "def show_red_crosses(img, corners):\n",
    "    # Create binary mask\n",
    "    peak_mask = np.zeros_like(img)\n",
    "    peak_mask[tuple(corners.T)] = True\n",
    "    # Create RGB image with red crosses in the corners\n",
    "    corners = cv.dilate(peak_mask, cv.getStructuringElement(cv.MORPH_CROSS, (5,5)))\n",
    "    output = cv.cvtColor(img, cv.COLOR_GRAY2RGB)\n",
    "    output[corners>0] = [255,0,0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fccf619a89f7d33c0e594d7458fff9d",
     "grade": false,
     "grade_id": "cell-9f8b33abefebf26c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell to test your `detect_corners` function on a test image that contains 12 corners. Your function should be able to detect them all correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2e493a9dbfd6d4de7d69cf4ed1cee98",
     "grade": true,
     "grade_id": "cell-1bd1d44c7627469a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7989c9918c38477181c3261099fb3364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33296cc69d6d4990bd21fdf7c52dde5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done, your function detected 12 corners in the image!\n",
      "However, it's your job to verify the correct location by looking at the picture.\n"
     ]
    }
   ],
   "source": [
    "# Create test image\n",
    "n = 51; r = n // 2\n",
    "test_img = np.zeros((n,n), dtype = np.uint8)\n",
    "test_img[r-15:r+16,r-5:r+6] = 255 \n",
    "test_img[r-5:r+6,r-15:r+16] = 255\n",
    "# Run corner detection\n",
    "detected_corners = detect_corners(test_img, 3, 0.5)\n",
    "# Visualize result\n",
    "plt.close('all')\n",
    "view = viewer(show_red_crosses(test_img,detected_corners))\n",
    "# Check that the number of detected corners is 12\n",
    "try:\n",
    "    assert len(detected_corners) == 12\n",
    "except AssertionError:\n",
    "    warnings.warn(f'\\n\\nSorry but your function detected {len(detected_corners)} corners instead of 12. \\\n",
    "                  Check your code and try again!')\n",
    "print(\"Well done, your function detected 12 corners in the image!\")\n",
    "print(\"However, it's your job to verify the correct location by looking at the picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dda34cf8930936fc3a6b96328b5a6d34",
     "grade": false,
     "grade_id": "cell-aba4cb8d5414a02a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, run the cell below to create an extra widget in the viewer and experiments with `L` and `T` to answer the upcoming MCQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f94791a63440d236f147fd1da71220d",
     "grade": false,
     "grade_id": "cell-8d18812943a18e17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2fdec164b84313bb665a72c303a8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(120, 547)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_code = cv.imread('images/qr_code.png', cv.IMREAD_GRAYSCALE)\n",
    "# Define Sliders and button\n",
    "T_slider = widgets.FloatSlider(value = 0.5, min = 0.05, max = 0.95, step=0.05, description = r'$T$')\n",
    "L_slider = widgets.IntSlider(value = 3, min = 1, max = 31, step = 2, description = r'$L$')\n",
    "button = widgets.Button(description = 'Detect Corners')\n",
    "# Define Callback function\n",
    "def corner_callback(img):\n",
    "    T = T_slider.value\n",
    "    L = L_slider.value\n",
    "    return show_red_crosses(img,detect_corners(img, L=L, T=T))\n",
    "# Start the visualization\n",
    "plt.close('all')\n",
    "view = viewer([harris_corner, corner, qr_code], new_widgets=[T_slider, L_slider, button], \n",
    "              callbacks=[corner_callback], widgets=True)\n",
    "harris_corner.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "caf4a198d3580e098aad86afd49bc0d7",
     "grade": false,
     "grade_id": "cell-6d0540daa8777c33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multiple Choice Question\n",
    "\n",
    "* Q1: In general, a higher $L$ leads to\n",
    "    1. more corners, because a higher area is covered,\n",
    "    2. less corners, because values need to be a local maximum in a larger area,\n",
    "    3. less corners, because less areas of the image are observed, or\n",
    "    4. more corners, because more areas of the image are observed.\n",
    "\n",
    "\n",
    "* Q2: In general, selecting a higher $T$ means that a corner has to be ... to be detected.\n",
    "    1. sharper, so that it's more defined,\n",
    "    2. rounder, so that it's less defined, or\n",
    "    3. more diffuse, so that it covers more area.\n",
    " \n",
    "In the next cell, modify the variables `answer_one` and `answer_two` to reflect your answers. The following two cells are for you to check that your answer is in the valid range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "566fcde765d6da8c4f3e9f1c36638c49",
     "grade": false,
     "grade_id": "cell-a511f33975192a71",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify these variables\n",
    "answer_one = 2\n",
    "answer_two = 1\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7014f883ae1bc6ef91ae0e396454e1a8",
     "grade": true,
     "grade_id": "cell-c163229713634c36",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert answer_one in [1, 2, 3, 4], 'Choose one of 1, 2, 3 or 4.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "704df682eb7dce9493cecc7a420f3792",
     "grade": true,
     "grade_id": "cell-a7246e682f4b085a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert answer_two in [1, 2, 3], 'Choose one of 1, 2 or 3.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cca343cc39bc5d64c0eeb732ff3c85d",
     "grade": false,
     "grade_id": "cell-574705ebfa0c74ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.C. *Advanced:* Isotropic filtering in the Fourier space (1 point)\n",
    "[Back to index](#Index)\n",
    "\n",
    "In the first exercise you were told to use the Sobel filter to compute the gradient for the structure tensor. As you have seen, this worked fine for everything we did until now. However, if we inspect the resulting orientation image more carefully, we can see some strange behavior. To illustrate this, let us first create a perfect test image, that contains (inside a certain radius of the image) exactly the same amount of pixels for every possible orientation. \n",
    "\n",
    "Run the next cell to create this test image and define two useful functions that we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c73f00adba36f4fe84feb54cdbd326af",
     "grade": false,
     "grade_id": "cell-c57f53ab2b89c278",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff10555e55c449a9c4a5f61edbfb749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed26e577ce7b4230814171a2da8df2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that creates the orientation test image. This image is perfectly isotropic\n",
    "def create_test_img(ny, nx):\n",
    "    # Minimum and maximum radial frequencies\n",
    "    fmin = 0.02; fmax = 8 * fmin;\n",
    "    # Center\n",
    "    hx = nx / 2; hy = ny / 2;\n",
    "    n = min(nx, ny);\n",
    "    def structure_func(j, i):\n",
    "        # Radius\n",
    "        r = np.sqrt((i - hx)**2 + (j - hy)**2);\n",
    "        # Radial envelope\n",
    "        u = 1.0 / (1.0 + np.exp((r - n * 0.45) / 2.0));\n",
    "        # Radial frequency profile\n",
    "        f = fmin + r * (fmax - fmin) / n;\n",
    "        # Radial modulating function\n",
    "        v = np.sin(np.pi * 2 * f * r);\n",
    "        return (1.0 + v * u) * 128\n",
    "    return np.fromfunction(structure_func, shape=(ny, nx)).astype(np.uint8)\n",
    "\n",
    "# Calculates the Fourier Transform\n",
    "def get_FT(img):\n",
    "    return np.fft.fftshift(np.fft.fft2(img))\n",
    "\n",
    "# Calculates the inverse Fourier Transform\n",
    "def get_iFT(img):\n",
    "    return np.fft.ifft2(np.fft.ifftshift(img)).real\n",
    "\n",
    "test_img = create_test_img(2048, 2048)\n",
    "plt.close('all')\n",
    "view = viewer(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0e155633812d145fd27d787349552a9",
     "grade": false,
     "grade_id": "cell-f02ea3b5e03dccc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, in theory, if we extract the orientations of a circular cut-out of this test image using the `orientation_features` function that you coded in [Part 2.A.]((#2.A.-Feature-calculation-(4-points))) and plot the distribution of these orientations, we should get a completely flat line. Let's see how it actually looks: run the cell below to display the orientation image and its distribution inside a circular cut-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2eebe53768b50ded41c8bbe8e9b38daf",
     "grade": false,
     "grade_id": "cell-575e17460ab73c7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66faf7606f1541b7a8721df809d2169a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89331cdbadf04e17ae9b6c9b330591fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39eb17fc92f43e8827eb9c962c56224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the old orientation\n",
    "orientation = orientation_features(test_img, sigma=5)[0]  \n",
    "# Generate the disk-shaped mask to only analize the colors inside the correct radius\n",
    "mask = np.fromfunction(lambda i, j: np.sqrt((i-orientation.shape[0]//2)**2 + (j-orientation.shape[1]//2)**2), \n",
    "                       shape=orientation.shape)    \n",
    "r = np.min(mask.shape)//2 - np.min(mask.shape)//20\n",
    "# Generate histograms using the mask\n",
    "hist, edg = np.histogram(orientation[mask < r], bins = 1000, range = (orientation.min(), orientation.max()))\n",
    "cent = (edg[0:-1] + edg[1:])/2\n",
    "# Display the masked orientation\n",
    "orientation[mask >= r] = -np.pi/2\n",
    "plt.close('all')\n",
    "view = viewer([orientation], title=['Orientation of the test image'], cmap='hsv', colorbar=True)\n",
    "# Display the angle distribution\n",
    "f = plt.figure(); plt.title('Orientation distribution'); plt.xlabel('Angle [rad]'); plt.ylabel('# pixels')\n",
    "plt.plot(cent, hist); plt.grid(); \n",
    "plt.xticks(ticks=[-np.pi/2,-np.pi/4,0,np.pi/4,np.pi/2],labels=[r'$-\\pi/2$',r'$-\\pi/4$',r'$0$',r'$\\pi/4$',r'$\\pi/2$']); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63fd1b284088e9e7f9f7c6a342bf0a82",
     "grade": false,
     "grade_id": "cell-3c6215e6afb796fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the first image, you will probably not see any problem since it's very hard to see it by just looking at the orientation image. However, in the distribution plot you can clearly see that some orientations are preferred over others. Specifically, ignoring the very sharp peaks at $0$, $\\pm\\frac{\\pi}{4}$ and $\\pm\\frac{\\pi}{2}$ (an artefact of having a limited number of pixels), there are more pixels with an orientation close to $0$ or $\\pm\\frac{\\pi}{2}$ and less pixels close to $\\pm\\frac{\\pi}{4}$. \n",
    "\n",
    "Now why could that be? If we look more closely at the Sobel filter masks $h_x$ and $h_y$\n",
    "\n",
    "$$h_x = \n",
    "\\begin{bmatrix} \n",
    "    1 & 0 & -1 \\\\\n",
    "    2 & \\boxed{0} & -2 \\\\ \n",
    "    1 & 0 & -1 \n",
    "\\end{bmatrix}\n",
    ",\\;\\;\\;\\;\n",
    "h_y = \n",
    "\\begin{bmatrix} \n",
    "    1 & 2 & 1 \\\\\n",
    "    0 & \\boxed{0} & 0 \\\\ \n",
    "    -1 & -2 & -1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "we can see that this filter is not isotropic, meaning it doesn't treat all directions equally. In this case, as we saw in the orientation distribution above, the horizontal and vertical edges are favored, resulting in a biased orientation distribution.\n",
    "\n",
    "In order to have a non-biased orientation detector, we need to re-implement the `structure_tensor` function using an isotropic gradient filter. There are several to choose from but in this exercise we will calculate the gradient using the Fourier property\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x} \\xrightarrow{\\mathcal{F}} j\\omega_x\\operatorname{F}(\\omega_x, \\omega_y), \\;\\;\\; \n",
    "\\frac{\\partial f(x,y)}{\\partial y} \\xrightarrow{\\mathcal{F}} j\\omega_y\\operatorname{F}(\\omega_x, \\omega_y)\\,.$$\n",
    "\n",
    "As you can see we only need to take the Fourier transform of our image and multiply it with either $j\\omega_x$ or $j\\omega_y$. Then we can take the inverse Fourier transform and we will have our derivatives in the $x$ and $y$ directions.\n",
    "\n",
    "**For 1 point**, implement the functions `get_w` and `structure_tensor_improved` in the cells below.\n",
    "\n",
    "The first step for you is to implement the function `get_w` that simply returns a vector $\\omega$ consisting of $m$ equidistant points going from $-\\frac{\\pi}{2}$ to $\\frac{\\pi}{2}$.\n",
    "\n",
    "This is a very simple function that will be used in `structure_tensor_improved` to generate the complex-valued vectors $j\\omega_x$ and $j\\omega_y$.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Hint:** Use the function [`np.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "504cc769f846822b7826b1ae5c8c18bf",
     "grade": false,
     "grade_id": "cell-ffc5cf790a58c210",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6337cc50f24f81ac7021e9704eefa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculates omega (w) of length m\n",
    "def get_w(m):\n",
    "    w = np.zeros(m)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    w = np.linspace(-np.pi/2, np.pi/2, num=m)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Visualize omega\n",
    "plt.close('all')\n",
    "plt.figure(\"Frequency vector\")\n",
    "plt.plot(get_w(100))\n",
    "plt.ylabel(r'$\\omega$'); plt.xlabel(\"Vector index\"); plt.grid()\n",
    "plt.yticks(ticks=[-np.pi/2,-np.pi/4,0,np.pi/4,np.pi/2],labels=[r'$-\\pi/2$',r'$-\\pi/4$',r'$0$',r'$\\pi/4$',r'$\\pi/2$']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b18e3242d4534b4dd91fe335de95759",
     "grade": false,
     "grade_id": "cell-046c459b454c6368",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell for a quick test on $\\omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcb050f20b4e1957a55eddc3d705d34c",
     "grade": true,
     "grade_id": "cell-6c8eb980d75d274f",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done, the function passed the sanity check.\n"
     ]
    }
   ],
   "source": [
    "# Perform test on a vector of 10 elements. We will check length, range, and a direct comparison with the solution\n",
    "w_out = get_w(10)\n",
    "assert len(w_out) == 10, f'The length of the output vector should be equal to the input parameter m. \\\n",
    "                           Expected length=10, your length={len(w_out)}.'\n",
    "assert np.min(w_out) == -np.pi/2 and np.max(w_out) == np.pi/2, \\\n",
    "f'Omega should go from -1.57079633 to 1.57079633 not from {np.min(w_out):.8f} to {np.max(w_out):.8f}!'\n",
    "np.testing.assert_array_almost_equal(w_out, [-1.57079633, -1.22173048, -0.87266463, -0.52359878, -0.17453293, \n",
    "                                     0.17453293,  0.52359878,  0.87266463,  1.22173048,  1.57079633], err_msg=\n",
    "                                     'Your range and length are correct, but the vector itself is not!')\n",
    "print('Well done, the function passed the sanity check.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfa7f83a1db94d80ae7252bf93f56c21",
     "grade": false,
     "grade_id": "cell-1a3bb11ed4697d88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the next cell, complete the function `structure_tensor_improved` by implementing the Fourier domain gradient filters as explained above. The code that generates the arrays $j\\omega_x$ and $j\\omega_y$ is already provided. It uses your function `get_w` to fill both the rows of the imaginary part of `jw_x` and the columns of the imaginary part of `jw_y` with the vector $\\omega$.\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Hint:** You can use the functions `get_FT(img)` to get the complex Fourier transform of `img` and `get_iFT(img_FT)` to get the inverse Fourier transform of `img_FT`. Those functions also take care of performing the correct shifting.\n",
    "</div>\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Hint:** Implementing these filters might be easier than you think! Or exactly as easy as you thought, but did not know why. Have a look at the NumPy broadcasting rules [here](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30cc8d63277784993425a209925fa313",
     "grade": false,
     "grade_id": "cell-ee9f298b5d3d60b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculates the elements Jxx, Jxy and Jyy of the structure tensor matrix\n",
    "def structure_tensor_improved(img, sigma):\n",
    "    Jxx = None\n",
    "    Jxy = None\n",
    "    Jyy = None\n",
    "    \n",
    "    # Generate the complex-numbered filters jw_x and jw_y\n",
    "    jw_x = 1j*get_w(img.shape[1])\n",
    "    jw_y = 1j*get_w(img.shape[0]).reshape((img.shape[0], 1))\n",
    "    \n",
    "    # Gradient calculation using jw_x and jw_y in the Fourier space\n",
    "    # YOUR CODE HERE\n",
    "    gradX = get_iFT(jw_x*get_FT(img))\n",
    "    gradY = get_iFT(jw_y*get_FT(img))\n",
    "    # Calculate fxx=gradXX, fxy=gradXY, fyy=gradYY and then Jxx, Jxy and Jyy from the gradients\n",
    "    # in exactly the same way as in the original structure tensor function (Part 1)\n",
    "    # YOUR CODE HERE\n",
    "    gradXX = gradX**2\n",
    "    gradXY = gradX*gradY\n",
    "    gradYY = gradY**2\n",
    "    \n",
    "    Jxx = cv.GaussianBlur(gradXX, ksize=(0,0), sigmaX=sigma, borderType=cv.BORDER_DEFAULT)\n",
    "    Jxy = cv.GaussianBlur(gradXY, ksize=(0,0), sigmaX=sigma, borderType=cv.BORDER_DEFAULT)\n",
    "    Jyy = cv.GaussianBlur(gradYY, ksize=(0,0), sigmaX=sigma, borderType=cv.BORDER_DEFAULT)\n",
    "    \n",
    "    return Jxx, Jxy, Jyy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff49a1bc50c06495708115e521b175e7",
     "grade": false,
     "grade_id": "cell-57e074d92e1518c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this part you will need to do the sanity check of the `structure_tensor_improved` function by eye. The cell below will run the function `orientation_features` twice on the `test_img`, once with the old `structure_tensor` and once with the improved version. \n",
    "\n",
    "You will be able to see the results in an *IPLabViwer*, where you will see the orientation feature for each version of the `structure_tensor` function, as well as the difference between both results. Once you are sure on the correctness of `structure_tensor_improved`, reflect on the difference and its origins. Moreover, you will see a plot of both orientation distributions. \n",
    "\n",
    "The displayed orientation feature is for you to see that you did not mess up the function, and you will most likely not be able to see much difference between the two versions. However, in the orientation distribution plot you should be able to clearly distinguish the Fourier version from the Sobel one. **The Fourier version should be completely flat (with exception of a few sharp spikes)** compared to the Sobel version that looks more like a sinusoidal wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e10ea9bc2de6ae49e446054e22918445",
     "grade": true,
     "grade_id": "cell-ab597977ff6f8830",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f51367d6aa4e2fac6588e9bf201c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db717b379e784ab7901205d5c601b63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the old orientation\n",
    "orientation = orientation_features(test_img, sigma=5)[0]\n",
    "# Replace the structure tensor function by the improved one\n",
    "old_ST = structure_tensor\n",
    "structure_tensor = structure_tensor_improved\n",
    "# Get the improved orientation\n",
    "try:\n",
    "    orientation_improved = orientation_features(test_img, sigma=5)[0]\n",
    "except:\n",
    "    # Handle errors in the code\n",
    "    structure_tensor = old_ST\n",
    "    raise\n",
    "# Restore the old structure tensor function\n",
    "structure_tensor = old_ST    \n",
    "\n",
    "# Generate the disk-shaped mask to only analize the colors inside the correct radius\n",
    "mask = np.fromfunction(lambda i, j: np.sqrt((i-orientation.shape[0]//2)**2 + (j-orientation.shape[1]//2)**2), \n",
    "                       shape=orientation.shape)    \n",
    "r = np.min(mask.shape)//2-np.min(mask.shape)//20\n",
    "\n",
    "# Generate histograms using the mask\n",
    "hist, edg = np.histogram(orientation[mask < r], bins = 1000, range = (orientation.min(), orientation.max()))\n",
    "hist_improved, edg_improved = np.histogram(orientation_improved[mask < r], bins = 1000, \n",
    "                                range = (orientation_improved.min(), orientation_improved.max()))\n",
    "cent = (edg[0:-1] + edg[1:])/2;\n",
    "cent_improved = (edg_improved[0:-1] + edg_improved[1:])/2;\n",
    "# Display the two masked orieantations\n",
    "orientation[mask >= r] = -np.pi/2\n",
    "orientation_improved[mask >= r] = -np.pi/2\n",
    "plt.close('all')\n",
    "image_list = [orientation, orientation_improved]\n",
    "title_list = ['Sobel orientation', 'Fourier orientation']\n",
    "view = viewer(image_list, title=title_list, cmap='hsv', colorbar=True, widgets=True)\n",
    "\n",
    "# Display the angle distribution\n",
    "plt.figure()\n",
    "plt.title('Orientation distribution'); plt.xlabel('Angle [rad]'); plt.ylabel('# pixels')\n",
    "plt.xticks(ticks=[-np.pi/2,-np.pi/4,0,np.pi/4,np.pi/2],labels=[r'$-\\pi/2$',r'$-\\pi/4$',r'$0$',r'$\\pi/4$',r'$\\pi/2$']);\n",
    "plt.plot(cent, hist)\n",
    "plt.plot(cent_improved, hist_improved)\n",
    "plt.legend(['Sobel orientation', 'Fourier orientation']); plt.grid(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a84872804ef184eb2fb8c650b32b07a7",
     "grade": false,
     "grade_id": "cell-501f4cf9de874311",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "<p><b>Congratulations on finishing the Orientation lab!!</b></p>\n",
    "<p>\n",
    "</div>\n",
    "Make sure to save your notebook (you might want to keep a copy on your personal computer) and upload it to <a href=\"https://moodle.epfl.ch/course/view.php?id=463\">Moodle</a>, in a zip file with other notebooks of this lab.\n",
    "</p>\n",
    "\n",
    "* Name the notebook: *SCIPER_2_Orientation.ipynb* (e.g. *123456_2_Orientation.ipynb*),\n",
    "* Name the zip file: *SCIPER_Orientation_lab.zip* (e.g. *123456_Orientation_lab.zip*).\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<h4>Feedback</h4>\n",
    "    <p style=\"margin:4px;\">\n",
    "    This is the first edition of the image-processing laboratories using Jupyter Notebooks running on Noto. Do not hand in before giving us your <a href=\"https://moodle.epfl.ch/mod/feedback/view.php?id=1135047\">feedback here!</a></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
